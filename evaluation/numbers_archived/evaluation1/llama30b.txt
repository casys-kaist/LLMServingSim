model: llama-30b, num requests: 1, total length: 10, prompt/kv_cache length: ['10']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['11']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['12']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['13']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['14']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['15']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['16']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['17']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['18']
model: llama-30b, num requests: 2, total length: 23, prompt/kv_cache length: ['19', '22']
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['20', '23']
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['21', '24']
model: llama-30b, num requests: 3, total length: 18, prompt/kv_cache length: ['22', '25', '16']
[0.5s] Avg Throughput: propmt: 64, generation: 26
model: llama-30b, num requests: 3, total length: 3, prompt/kv_cache length: ['23', '26', '17']
model: llama-30b, num requests: 4, total length: 7, prompt/kv_cache length: ['24', '27', '18', '4']
model: llama-30b, num requests: 7, total length: 47, prompt/kv_cache length: ['25', '28', '19', '5', '13', '15', '15']
model: llama-30b, num requests: 7, total length: 7, prompt/kv_cache length: ['26', '29', '20', '6', '14', '16', '16']
model: llama-30b, num requests: 7, total length: 7, prompt/kv_cache length: ['27', '30', '21', '7', '15', '17', '17']
model: llama-30b, num requests: 7, total length: 7, prompt/kv_cache length: ['28', '31', '22', '8', '16', '18', '18']
model: llama-30b, num requests: 8, total length: 9, prompt/kv_cache length: ['29', '32', '23', '9', '17', '19', '19', '2']
model: llama-30b, num requests: 8, total length: 8, prompt/kv_cache length: ['30', '33', '24', '10', '18', '20', '20', '3']
model: llama-30b, num requests: 9, total length: 26, prompt/kv_cache length: ['31', '34', '25', '11', '19', '21', '21', '4', '18']
model: llama-30b, num requests: 9, total length: 9, prompt/kv_cache length: ['32', '35', '26', '12', '20', '22', '22', '5', '19']
[1.0s] Avg Throughput: propmt: 166, generation: 112
model: llama-30b, num requests: 11, total length: 97, prompt/kv_cache length: ['33', '36', '27', '13', '21', '23', '23', '6', '20', '5', '83']
model: llama-30b, num requests: 11, total length: 11, prompt/kv_cache length: ['34', '37', '28', '14', '22', '24', '24', '7', '21', '6', '84']
model: llama-30b, num requests: 11, total length: 11, prompt/kv_cache length: ['35', '38', '29', '15', '23', '25', '25', '8', '22', '7', '85']
model: llama-30b, num requests: 10, total length: 10, prompt/kv_cache length: ['36', '39', '30', '24', '26', '26', '9', '23', '8', '86']
model: llama-30b, num requests: 10, total length: 10, prompt/kv_cache length: ['37', '40', '31', '25', '27', '27', '10', '24', '9', '87']
model: llama-30b, num requests: 10, total length: 17, prompt/kv_cache length: ['38', '41', '32', '26', '28', '28', '25', '10', '88', '8']
model: llama-30b, num requests: 10, total length: 10, prompt/kv_cache length: ['39', '42', '33', '27', '29', '29', '26', '11', '89', '9']
[1.5s] Avg Throughput: propmt: 192, generation: 138
model: llama-30b, num requests: 10, total length: 10, prompt/kv_cache length: ['40', '43', '34', '28', '30', '30', '27', '12', '90', '10']
model: llama-30b, num requests: 11, total length: 24, prompt/kv_cache length: ['41', '44', '35', '29', '31', '31', '28', '13', '91', '11', '14']
model: llama-30b, num requests: 13, total length: 28, prompt/kv_cache length: ['42', '45', '36', '30', '32', '32', '29', '14', '92', '12', '15', '14', '3']
model: llama-30b, num requests: 15, total length: 44, prompt/kv_cache length: ['43', '46', '37', '31', '33', '33', '30', '15', '93', '13', '16', '15', '4', '8', '23']
model: llama-30b, num requests: 16, total length: 33, prompt/kv_cache length: ['44', '47', '38', '32', '34', '34', '31', '16', '94', '14', '17', '16', '5', '9', '24', '18']
model: llama-30b, num requests: 17, total length: 46, prompt/kv_cache length: ['45', '48', '39', '33', '35', '35', '32', '17', '95', '15', '18', '17', '6', '10', '25', '19', '30']
model: llama-30b, num requests: 18, total length: 42, prompt/kv_cache length: ['46', '49', '40', '34', '36', '36', '33', '18', '96', '16', '19', '18', '7', '11', '26', '20', '31', '25']
model: llama-30b, num requests: 20, total length: 36, prompt/kv_cache length: ['47', '50', '41', '35', '37', '37', '34', '19', '97', '17', '20', '19', '8', '12', '27', '21', '32', '26', '8', '10']
[2.0s] Avg Throughput: propmt: 270, generation: 204
model: llama-30b, num requests: 22, total length: 92, prompt/kv_cache length: ['48', '51', '42', '36', '38', '38', '35', '20', '98', '18', '21', '20', '9', '13', '28', '22', '33', '27', '9', '11', '4', '68']
model: llama-30b, num requests: 23, total length: 29, prompt/kv_cache length: ['49', '52', '43', '37', '39', '39', '36', '21', '99', '19', '22', '21', '10', '14', '29', '23', '34', '28', '10', '12', '5', '69', '7']
model: llama-30b, num requests: 23, total length: 23, prompt/kv_cache length: ['50', '53', '44', '38', '40', '40', '37', '22', '100', '20', '23', '22', '11', '15', '30', '24', '35', '29', '11', '13', '6', '70', '8']
model: llama-30b, num requests: 25, total length: 49, prompt/kv_cache length: ['51', '54', '45', '39', '41', '41', '38', '23', '101', '21', '24', '23', '12', '16', '31', '25', '36', '30', '12', '14', '7', '71', '9', '10', '16']
model: llama-30b, num requests: 26, total length: 33, prompt/kv_cache length: ['52', '55', '46', '40', '42', '42', '39', '24', '102', '22', '25', '24', '13', '17', '32', '26', '37', '31', '13', '15', '8', '72', '10', '11', '17', '8']
model: llama-30b, num requests: 26, total length: 26, prompt/kv_cache length: ['53', '56', '47', '41', '43', '43', '40', '25', '103', '23', '26', '25', '14', '18', '33', '27', '38', '32', '14', '16', '9', '73', '11', '12', '18', '9']
model: llama-30b, num requests: 28, total length: 86, prompt/kv_cache length: ['54', '57', '48', '42', '44', '44', '41', '26', '104', '24', '27', '26', '15', '19', '34', '28', '39', '33', '15', '17', '10', '74', '12', '13', '19', '10', '47', '13']
[2.5s] Avg Throughput: propmt: 262, generation: 314
model: llama-30b, num requests: 31, total length: 105, prompt/kv_cache length: ['55', '58', '49', '43', '45', '45', '42', '27', '105', '25', '28', '27', '16', '20', '35', '29', '40', '34', '16', '18', '11', '75', '13', '14', '20', '11', '48', '14', '26', '8', '43']
model: llama-30b, num requests: 31, total length: 31, prompt/kv_cache length: ['56', '59', '50', '44', '46', '46', '43', '28', '106', '26', '29', '28', '17', '21', '36', '30', '41', '35', '17', '19', '12', '76', '14', '15', '21', '12', '49', '15', '27', '9', '44']
model: llama-30b, num requests: 30, total length: 30, prompt/kv_cache length: ['57', '60', '51', '45', '47', '47', '44', '29', '107', '27', '30', '29', '18', '22', '37', '31', '42', '36', '18', '13', '77', '15', '16', '22', '13', '50', '16', '28', '10', '45']
model: llama-30b, num requests: 31, total length: 73, prompt/kv_cache length: ['58', '61', '52', '46', '48', '48', '45', '30', '108', '28', '31', '30', '19', '23', '38', '32', '43', '37', '19', '14', '78', '16', '17', '23', '14', '51', '17', '29', '11', '46', '43']
model: llama-30b, num requests: 30, total length: 30, prompt/kv_cache length: ['59', '62', '53', '47', '49', '49', '46', '109', '29', '32', '31', '20', '24', '39', '33', '44', '38', '20', '15', '79', '17', '18', '24', '15', '52', '18', '30', '12', '47', '44']
[3.0s] Avg Throughput: propmt: 360, generation: 290
model: llama-30b, num requests: 30, total length: 30, prompt/kv_cache length: ['60', '63', '54', '48', '50', '50', '47', '110', '30', '33', '32', '21', '25', '40', '34', '45', '39', '21', '16', '80', '18', '19', '25', '16', '53', '19', '31', '13', '48', '45']
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['61', '64', '55', '49', '51', '51', '48', '111', '31', '34', '33', '22', '26', '41', '35', '46', '40', '22', '17', '81', '19', '26', '17', '54', '20', '32', '14', '49', '46']
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['62', '65', '56', '50', '52', '52', '49', '112', '32', '35', '34', '23', '27', '42', '36', '47', '41', '23', '18', '82', '20', '27', '18', '55', '21', '33', '15', '50', '47']
model: llama-30b, num requests: 30, total length: 46, prompt/kv_cache length: ['63', '66', '57', '51', '53', '53', '50', '113', '33', '36', '35', '24', '28', '43', '37', '48', '42', '24', '19', '83', '21', '28', '19', '56', '22', '34', '16', '51', '48', '17']
model: llama-30b, num requests: 30, total length: 30, prompt/kv_cache length: ['64', '67', '58', '52', '54', '54', '51', '114', '34', '37', '36', '25', '29', '44', '38', '49', '43', '25', '20', '84', '22', '29', '20', '57', '23', '35', '17', '52', '49', '18']
model: llama-30b, num requests: 31, total length: 38, prompt/kv_cache length: ['65', '68', '59', '53', '55', '55', '52', '115', '35', '38', '37', '26', '30', '45', '39', '50', '44', '26', '21', '85', '23', '30', '21', '58', '24', '36', '18', '53', '50', '19', '8']
model: llama-30b, num requests: 32, total length: 40, prompt/kv_cache length: ['66', '69', '60', '54', '56', '56', '53', '116', '36', '39', '38', '27', '31', '46', '40', '45', '27', '22', '86', '24', '31', '22', '59', '25', '37', '19', '54', '51', '20', '9', '1', '9']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['67', '70', '61', '55', '57', '57', '54', '117', '37', '40', '39', '28', '32', '47', '41', '46', '28', '23', '87', '25', '32', '23', '60', '26', '38', '20', '55', '52', '21', '10', '2', '10']
[3.5s] Avg Throughput: propmt: 70, generation: 474
model: llama-30b, num requests: 33, total length: 34, prompt/kv_cache length: ['68', '71', '62', '56', '58', '58', '55', '118', '38', '41', '40', '29', '33', '48', '42', '47', '29', '24', '88', '26', '33', '24', '61', '27', '39', '21', '56', '53', '22', '11', '3', '11', '2']
model: llama-30b, num requests: 35, total length: 137, prompt/kv_cache length: ['69', '72', '63', '57', '59', '59', '56', '119', '39', '42', '41', '30', '34', '49', '43', '48', '30', '25', '89', '27', '34', '25', '62', '28', '40', '22', '57', '54', '23', '12', '4', '12', '3', '1', '103']
model: llama-30b, num requests: 38, total length: 120, prompt/kv_cache length: ['70', '73', '64', '58', '60', '60', '57', '120', '40', '43', '42', '31', '35', '50', '44', '49', '31', '26', '90', '28', '35', '26', '63', '29', '41', '23', '58', '55', '24', '13', '5', '13', '4', '2', '104', '63', '5', '17']
[4.0s] Avg Throughput: propmt: 212, generation: 194
model: llama-30b, num requests: 38, total length: 62, prompt/kv_cache length: ['71', '74', '65', '59', '61', '61', '58', '121', '41', '44', '43', '32', '36', '51', '45', '50', '32', '27', '91', '29', '36', '27', '64', '30', '42', '24', '59', '25', '14', '6', '14', '5', '3', '64', '6', '18', '6', '20']
model: llama-30b, num requests: 39, total length: 74, prompt/kv_cache length: ['72', '75', '66', '60', '62', '62', '59', '122', '42', '45', '44', '33', '37', '52', '46', '51', '33', '28', '92', '30', '37', '28', '65', '31', '43', '25', '60', '26', '15', '7', '15', '6', '4', '65', '7', '19', '7', '21', '36']
model: llama-30b, num requests: 40, total length: 148, prompt/kv_cache length: ['73', '76', '67', '61', '63', '63', '60', '123', '43', '46', '45', '34', '38', '53', '47', '52', '34', '29', '93', '31', '38', '29', '66', '32', '44', '26', '61', '27', '16', '8', '16', '7', '5', '66', '8', '20', '8', '22', '37', '109']
[4.5s] Avg Throughput: propmt: 294, generation: 218
model: llama-30b, num requests: 42, total length: 122, prompt/kv_cache length: ['74', '77', '68', '62', '64', '64', '61', '124', '44', '47', '46', '35', '39', '54', '48', '53', '35', '30', '94', '32', '39', '30', '67', '33', '45', '27', '62', '28', '17', '9', '17', '8', '6', '67', '9', '21', '9', '23', '38', '110', '9', '73']
model: llama-30b, num requests: 43, total length: 54, prompt/kv_cache length: ['75', '78', '69', '63', '65', '65', '62', '125', '45', '48', '47', '36', '40', '55', '49', '54', '36', '31', '95', '33', '40', '31', '68', '34', '46', '28', '63', '29', '18', '10', '18', '9', '7', '68', '10', '22', '10', '24', '39', '111', '10', '74', '12']
model: llama-30b, num requests: 43, total length: 46, prompt/kv_cache length: ['76', '79', '70', '64', '66', '66', '63', '126', '46', '49', '48', '37', '41', '56', '50', '55', '32', '96', '34', '41', '32', '69', '35', '47', '29', '64', '30', '19', '11', '19', '10', '8', '69', '11', '23', '11', '25', '40', '112', '11', '75', '13', '4']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['77', '80', '71', '65', '67', '67', '64', '127', '47', '50', '49', '38', '42', '57', '51', '56', '33', '97', '35', '42', '33', '70', '36', '48', '30', '65', '31', '20', '12', '20', '11', '9', '70', '12', '24', '12', '26', '41', '113', '12', '76', '14', '5']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['78', '81', '72', '66', '68', '68', '65', '128', '48', '51', '50', '39', '43', '58', '52', '57', '34', '98', '36', '43', '34', '71', '37', '49', '31', '66', '32', '21', '13', '21', '12', '10', '71', '13', '25', '13', '27', '42', '114', '13', '77', '15', '6']
[5.0s] Avg Throughput: propmt: 414, generation: 412
model: llama-30b, num requests: 44, total length: 59, prompt/kv_cache length: ['79', '82', '73', '67', '69', '69', '66', '129', '49', '52', '51', '40', '44', '59', '53', '58', '35', '99', '37', '44', '35', '72', '38', '50', '32', '67', '33', '22', '14', '22', '13', '11', '72', '14', '26', '14', '28', '43', '115', '14', '78', '16', '7', '16']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['83', '74', '68', '70', '70', '67', '130', '50', '53', '52', '41', '45', '60', '54', '59', '36', '100', '38', '45', '36', '73', '39', '51', '33', '68', '34', '23', '15', '23', '14', '12', '73', '15', '27', '15', '29', '44', '116', '15', '79', '17', '8', '17']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['84', '75', '69', '71', '71', '68', '131', '51', '54', '53', '42', '46', '61', '55', '60', '37', '101', '39', '46', '37', '74', '40', '52', '34', '69', '35', '24', '16', '24', '15', '13', '74', '16', '28', '16', '30', '45', '117', '16', '80', '18', '9', '18']
model: llama-30b, num requests: 43, total length: 91, prompt/kv_cache length: ['85', '76', '70', '72', '72', '69', '132', '52', '55', '54', '43', '47', '62', '56', '61', '38', '102', '40', '47', '38', '75', '41', '53', '70', '36', '25', '17', '25', '16', '14', '75', '17', '29', '17', '31', '46', '118', '17', '81', '19', '10', '19', '49']
model: llama-30b, num requests: 44, total length: 52, prompt/kv_cache length: ['86', '77', '71', '73', '73', '70', '133', '53', '56', '55', '44', '48', '63', '57', '62', '39', '103', '41', '48', '39', '76', '42', '54', '71', '37', '26', '18', '26', '17', '15', '76', '18', '30', '18', '32', '47', '119', '18', '82', '20', '11', '20', '50', '9']
model: llama-30b, num requests: 44, total length: 44, prompt/kv_cache length: ['87', '78', '72', '74', '74', '71', '134', '54', '57', '56', '45', '49', '64', '58', '63', '40', '104', '42', '49', '40', '77', '43', '55', '72', '38', '27', '19', '27', '18', '16', '77', '19', '31', '19', '33', '48', '120', '19', '83', '21', '12', '21', '51', '10']
[5.5s] Avg Throughput: propmt: 148, generation: 514
model: llama-30b, num requests: 42, total length: 42, prompt/kv_cache length: ['88', '79', '73', '75', '75', '72', '135', '55', '58', '57', '46', '50', '65', '59', '64', '41', '105', '43', '50', '41', '44', '56', '73', '39', '28', '20', '28', '19', '17', '78', '20', '32', '20', '34', '49', '121', '20', '84', '22', '22', '52', '11']
model: llama-30b, num requests: 42, total length: 42, prompt/kv_cache length: ['89', '80', '74', '76', '76', '73', '136', '56', '59', '58', '47', '51', '66', '60', '65', '42', '106', '44', '51', '42', '45', '57', '74', '40', '29', '21', '29', '20', '18', '79', '21', '33', '21', '35', '50', '122', '21', '85', '23', '23', '53', '12']
model: llama-30b, num requests: 46, total length: 220, prompt/kv_cache length: ['90', '81', '75', '77', '77', '74', '137', '57', '60', '59', '48', '52', '67', '61', '66', '43', '107', '45', '52', '43', '46', '58', '75', '41', '30', '22', '30', '21', '19', '80', '22', '34', '22', '36', '51', '123', '22', '86', '24', '24', '54', '13', '39', '91', '24', '24']
model: llama-30b, num requests: 48, total length: 202, prompt/kv_cache length: ['91', '82', '76', '78', '75', '138', '58', '61', '60', '49', '53', '68', '62', '67', '44', '108', '46', '53', '44', '47', '59', '76', '42', '31', '23', '31', '22', '20', '81', '23', '35', '23', '37', '52', '124', '23', '87', '25', '25', '55', '14', '40', '92', '25', '25', '120', '18', '19']
[6.0s] Avg Throughput: propmt: 356, generation: 340
model: llama-30b, num requests: 50, total length: 123, prompt/kv_cache length: ['92', '83', '77', '79', '139', '59', '62', '61', '50', '54', '69', '63', '68', '45', '109', '47', '54', '45', '48', '60', '77', '43', '32', '24', '32', '82', '24', '36', '24', '38', '53', '125', '24', '88', '26', '26', '56', '15', '41', '93', '26', '26', '121', '19', '20', '14', '6', '27', '12', '19']
model: llama-30b, num requests: 51, total length: 89, prompt/kv_cache length: ['93', '84', '78', '80', '140', '60', '63', '62', '51', '55', '70', '64', '69', '46', '110', '48', '55', '46', '49', '61', '78', '44', '33', '25', '33', '83', '25', '37', '25', '39', '54', '126', '25', '89', '27', '27', '57', '16', '42', '94', '27', '122', '20', '21', '15', '7', '28', '13', '20', '36', '4']
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['94', '79', '81', '141', '61', '64', '63', '52', '56', '71', '65', '70', '47', '111', '49', '56', '47', '50', '62', '79', '45', '34', '26', '34', '84', '26', '38', '26', '40', '55', '127', '26', '90', '28', '28', '58', '17', '43', '28', '123', '21', '22', '16', '8', '29', '14', '21', '37', '5']
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['95', '80', '82', '142', '62', '65', '64', '53', '57', '72', '66', '71', '48', '112', '50', '57', '48', '51', '63', '80', '46', '35', '27', '35', '85', '27', '39', '27', '41', '56', '128', '27', '91', '29', '29', '59', '18', '44', '29', '124', '22', '23', '17', '9', '30', '15', '22', '38', '6']
[6.5s] Avg Throughput: propmt: 550, generation: 376
model: llama-30b, num requests: 48, total length: 48, prompt/kv_cache length: ['96', '81', '83', '143', '63', '66', '65', '54', '58', '73', '67', '72', '49', '113', '51', '58', '49', '52', '64', '81', '47', '36', '36', '86', '28', '40', '28', '42', '57', '129', '28', '92', '30', '30', '60', '19', '45', '30', '125', '23', '24', '18', '10', '31', '16', '23', '39', '7']
model: llama-30b, num requests: 48, total length: 48, prompt/kv_cache length: ['97', '82', '84', '144', '64', '67', '66', '55', '59', '74', '68', '73', '50', '114', '52', '59', '50', '53', '65', '82', '48', '37', '37', '87', '29', '41', '29', '43', '58', '130', '29', '93', '31', '31', '61', '20', '46', '31', '126', '24', '25', '19', '11', '32', '17', '24', '40', '8']
model: llama-30b, num requests: 48, total length: 48, prompt/kv_cache length: ['98', '83', '85', '145', '65', '68', '67', '56', '60', '75', '69', '74', '51', '115', '53', '60', '51', '54', '66', '83', '49', '38', '38', '88', '30', '42', '30', '44', '59', '131', '30', '94', '32', '32', '62', '21', '47', '32', '127', '25', '26', '20', '12', '33', '18', '25', '41', '9']
model: llama-30b, num requests: 49, total length: 55, prompt/kv_cache length: ['99', '84', '86', '146', '66', '69', '68', '57', '61', '76', '70', '75', '52', '116', '54', '61', '52', '55', '67', '84', '50', '39', '39', '89', '31', '43', '31', '45', '60', '132', '31', '95', '33', '33', '63', '22', '48', '33', '128', '26', '27', '21', '13', '34', '19', '26', '42', '10', '7']
model: llama-30b, num requests: 48, total length: 96, prompt/kv_cache length: ['100', '85', '87', '67', '70', '69', '58', '62', '77', '71', '76', '53', '117', '55', '62', '53', '56', '68', '85', '51', '40', '40', '90', '32', '44', '32', '61', '133', '32', '96', '34', '34', '64', '23', '49', '34', '129', '27', '28', '22', '14', '35', '20', '27', '43', '11', '8', '49']
model: llama-30b, num requests: 48, total length: 90, prompt/kv_cache length: ['101', '86', '88', '68', '71', '70', '59', '63', '78', '72', '77', '118', '56', '63', '54', '57', '69', '86', '52', '41', '41', '91', '33', '45', '33', '62', '134', '33', '97', '35', '65', '24', '50', '35', '130', '28', '29', '23', '15', '36', '21', '28', '44', '12', '9', '50', '13', '31']
[7.0s] Avg Throughput: propmt: 112, generation: 576
model: llama-30b, num requests: 49, total length: 59, prompt/kv_cache length: ['102', '87', '89', '69', '72', '71', '60', '64', '79', '73', '78', '119', '57', '64', '55', '58', '70', '87', '53', '42', '42', '92', '34', '46', '34', '63', '135', '34', '98', '36', '66', '25', '51', '36', '131', '29', '30', '24', '16', '37', '22', '29', '45', '13', '10', '51', '14', '32', '11']
model: llama-30b, num requests: 46, total length: 46, prompt/kv_cache length: ['103', '88', '90', '70', '73', '72', '61', '65', '80', '74', '120', '58', '65', '56', '59', '71', '88', '54', '43', '43', '93', '35', '35', '64', '136', '35', '99', '37', '67', '26', '52', '37', '132', '30', '31', '25', '17', '38', '23', '30', '46', '14', '11', '52', '15', '33']
model: llama-30b, num requests: 47, total length: 68, prompt/kv_cache length: ['104', '89', '91', '71', '74', '73', '62', '66', '81', '75', '121', '59', '66', '57', '60', '72', '89', '55', '44', '44', '94', '36', '36', '65', '137', '36', '100', '38', '68', '27', '53', '38', '133', '31', '32', '26', '18', '39', '24', '31', '47', '15', '12', '53', '16', '34', '22']
model: llama-30b, num requests: 48, total length: 61, prompt/kv_cache length: ['105', '90', '92', '72', '75', '74', '63', '67', '82', '76', '122', '60', '67', '58', '61', '73', '90', '56', '45', '45', '95', '37', '37', '66', '138', '37', '101', '39', '69', '28', '54', '39', '134', '32', '33', '27', '19', '40', '25', '32', '48', '16', '13', '54', '17', '35', '23', '14']
model: llama-30b, num requests: 47, total length: 47, prompt/kv_cache length: ['106', '91', '93', '73', '76', '75', '64', '68', '77', '123', '61', '68', '59', '62', '74', '91', '57', '46', '46', '96', '38', '38', '67', '139', '38', '102', '40', '70', '29', '55', '40', '135', '33', '34', '28', '20', '41', '26', '33', '49', '17', '14', '55', '18', '36', '24', '15']
model: llama-30b, num requests: 49, total length: 76, prompt/kv_cache length: ['107', '92', '94', '74', '77', '76', '65', '69', '78', '124', '62', '69', '60', '63', '75', '92', '58', '47', '47', '97', '39', '39', '68', '140', '39', '103', '41', '71', '30', '56', '41', '136', '34', '35', '29', '21', '42', '27', '34', '50', '18', '15', '56', '19', '37', '25', '16', '14', '15']
[7.5s] Avg Throughput: propmt: 182, generation: 560
model: llama-30b, num requests: 51, total length: 86, prompt/kv_cache length: ['108', '93', '95', '75', '78', '77', '66', '70', '79', '125', '63', '70', '61', '64', '76', '93', '59', '48', '48', '98', '40', '40', '69', '141', '40', '104', '42', '72', '31', '57', '42', '137', '35', '36', '30', '22', '43', '28', '35', '51', '19', '16', '57', '20', '38', '26', '17', '15', '16', '32', '5']
model: llama-30b, num requests: 51, total length: 51, prompt/kv_cache length: ['109', '94', '96', '76', '79', '78', '67', '71', '80', '126', '64', '71', '62', '65', '77', '94', '60', '49', '49', '99', '41', '41', '70', '142', '41', '105', '43', '73', '32', '58', '43', '138', '36', '37', '31', '23', '44', '29', '36', '52', '20', '17', '58', '21', '39', '27', '18', '16', '17', '33', '6']
model: llama-30b, num requests: 52, total length: 94, prompt/kv_cache length: ['110', '95', '97', '77', '80', '79', '68', '72', '81', '127', '65', '72', '63', '66', '78', '95', '61', '50', '50', '100', '42', '42', '71', '143', '42', '106', '44', '74', '33', '59', '44', '139', '37', '38', '32', '24', '45', '30', '37', '53', '21', '18', '59', '22', '40', '28', '19', '17', '18', '34', '7', '43']
model: llama-30b, num requests: 56, total length: 100, prompt/kv_cache length: ['111', '96', '98', '78', '81', '80', '69', '73', '82', '128', '66', '73', '64', '67', '79', '96', '62', '51', '51', '101', '43', '43', '72', '144', '43', '107', '45', '75', '34', '60', '45', '140', '38', '39', '33', '25', '46', '31', '38', '54', '22', '19', '60', '23', '41', '29', '20', '18', '19', '35', '8', '44', '16', '6', '21', '5']
model: llama-30b, num requests: 56, total length: 65, prompt/kv_cache length: ['112', '97', '79', '82', '81', '70', '74', '83', '129', '67', '74', '65', '68', '80', '97', '63', '52', '52', '102', '44', '44', '73', '145', '44', '108', '46', '76', '35', '61', '46', '141', '39', '40', '34', '26', '47', '32', '39', '55', '23', '20', '61', '24', '42', '30', '21', '19', '20', '36', '9', '45', '17', '7', '22', '6', '10']
[8.0s] Avg Throughput: propmt: 314, generation: 500
model: llama-30b, num requests: 56, total length: 56, prompt/kv_cache length: ['113', '98', '80', '83', '82', '71', '75', '84', '130', '68', '75', '66', '69', '81', '98', '64', '53', '53', '103', '45', '45', '74', '146', '45', '109', '47', '77', '36', '62', '47', '142', '40', '41', '35', '27', '48', '33', '40', '56', '24', '62', '25', '43', '31', '22', '20', '21', '37', '10', '46', '18', '8', '23', '7', '11', '1']
model: llama-30b, num requests: 57, total length: 72, prompt/kv_cache length: ['114', '99', '81', '84', '83', '72', '76', '85', '131', '69', '76', '67', '70', '82', '99', '65', '54', '54', '104', '46', '46', '75', '147', '46', '110', '48', '78', '37', '63', '48', '143', '41', '42', '36', '28', '49', '34', '41', '57', '25', '63', '26', '44', '32', '23', '21', '22', '38', '11', '47', '19', '9', '24', '8', '12', '2', '16']
model: llama-30b, num requests: 55, total length: 55, prompt/kv_cache length: ['115', '100', '82', '85', '84', '73', '77', '86', '132', '70', '77', '68', '71', '83', '100', '66', '55', '105', '47', '47', '76', '148', '47', '111', '49', '79', '38', '64', '49', '144', '42', '43', '37', '50', '35', '42', '58', '26', '64', '27', '45', '33', '24', '22', '23', '39', '12', '48', '20', '10', '25', '9', '13', '3', '17']
model: llama-30b, num requests: 54, total length: 54, prompt/kv_cache length: ['116', '101', '83', '86', '85', '74', '78', '87', '133', '71', '78', '69', '72', '84', '101', '67', '56', '106', '48', '48', '77', '149', '48', '50', '80', '39', '65', '50', '145', '43', '44', '38', '51', '36', '43', '59', '27', '65', '28', '46', '34', '25', '23', '24', '40', '13', '49', '21', '11', '26', '10', '14', '4', '18']
model: llama-30b, num requests: 55, total length: 120, prompt/kv_cache length: ['117', '102', '84', '87', '86', '75', '79', '88', '134', '72', '79', '70', '73', '85', '102', '68', '57', '107', '49', '49', '78', '150', '49', '51', '81', '40', '66', '51', '146', '44', '45', '39', '52', '37', '44', '60', '28', '66', '29', '47', '35', '26', '24', '25', '41', '14', '50', '22', '12', '27', '11', '15', '5', '19', '66']
[8.5s] Avg Throughput: propmt: 54, generation: 550
model: llama-30b, num requests: 58, total length: 88, prompt/kv_cache length: ['118', '103', '85', '88', '87', '76', '80', '89', '135', '73', '80', '71', '74', '86', '103', '69', '58', '108', '50', '50', '79', '151', '50', '52', '82', '41', '67', '52', '147', '45', '46', '40', '53', '38', '45', '61', '29', '67', '30', '48', '36', '27', '25', '26', '42', '15', '51', '23', '13', '28', '12', '16', '6', '20', '67', '9', '13', '11']
model: llama-30b, num requests: 58, total length: 58, prompt/kv_cache length: ['119', '104', '86', '89', '88', '77', '81', '90', '136', '74', '81', '72', '75', '87', '104', '70', '59', '109', '51', '51', '80', '152', '51', '53', '83', '42', '68', '53', '148', '46', '47', '41', '54', '39', '46', '62', '30', '68', '31', '49', '37', '28', '26', '27', '43', '16', '52', '24', '14', '29', '13', '17', '7', '21', '68', '10', '14', '12']
model: llama-30b, num requests: 58, total length: 69, prompt/kv_cache length: ['120', '105', '87', '90', '89', '78', '82', '91', '137', '75', '82', '73', '76', '88', '105', '71', '110', '52', '52', '81', '153', '52', '54', '84', '43', '69', '54', '149', '47', '48', '42', '55', '40', '47', '63', '31', '69', '32', '50', '38', '29', '27', '28', '44', '17', '53', '25', '15', '30', '14', '18', '8', '22', '69', '11', '15', '13', '12']
model: llama-30b, num requests: 58, total length: 64, prompt/kv_cache length: ['121', '106', '88', '91', '90', '79', '83', '92', '138', '76', '83', '74', '77', '89', '106', '72', '111', '53', '53', '82', '154', '53', '55', '85', '44', '55', '150', '48', '49', '43', '56', '41', '48', '64', '32', '70', '33', '51', '39', '30', '28', '29', '45', '18', '54', '26', '16', '31', '15', '19', '9', '23', '70', '12', '16', '14', '13', '7']
model: llama-30b, num requests: 58, total length: 58, prompt/kv_cache length: ['122', '107', '89', '92', '91', '80', '84', '93', '139', '77', '84', '75', '78', '90', '107', '73', '112', '54', '54', '83', '155', '54', '56', '86', '45', '56', '151', '49', '50', '44', '57', '42', '49', '65', '33', '71', '34', '52', '40', '31', '29', '30', '46', '19', '55', '27', '17', '32', '16', '20', '10', '24', '71', '13', '17', '15', '14', '8']
[9.0s] Avg Throughput: propmt: 236, generation: 562
model: llama-30b, num requests: 58, total length: 82, prompt/kv_cache length: ['123', '108', '90', '93', '92', '81', '85', '94', '140', '78', '85', '76', '79', '91', '108', '74', '113', '55', '55', '84', '156', '55', '57', '46', '152', '50', '51', '45', '58', '43', '50', '66', '34', '72', '35', '53', '41', '32', '30', '31', '47', '20', '56', '28', '18', '33', '17', '21', '11', '25', '72', '14', '18', '16', '15', '9', '11', '15']
model: llama-30b, num requests: 58, total length: 79, prompt/kv_cache length: ['124', '91', '94', '93', '82', '86', '95', '141', '79', '86', '77', '80', '92', '109', '75', '114', '56', '56', '85', '157', '56', '58', '47', '153', '51', '52', '46', '59', '44', '51', '67', '35', '73', '36', '54', '42', '33', '31', '32', '48', '21', '57', '29', '19', '34', '18', '22', '12', '26', '73', '15', '19', '17', '16', '10', '12', '16', '22']
model: llama-30b, num requests: 61, total length: 123, prompt/kv_cache length: ['125', '92', '95', '94', '83', '87', '96', '142', '80', '87', '78', '81', '93', '110', '76', '115', '57', '57', '86', '158', '57', '59', '48', '154', '52', '53', '47', '60', '45', '52', '68', '36', '74', '37', '55', '43', '34', '32', '33', '49', '22', '58', '30', '20', '35', '19', '23', '13', '27', '74', '16', '20', '18', '17', '11', '13', '17', '23', '42', '5', '18']
model: llama-30b, num requests: 60, total length: 60, prompt/kv_cache length: ['93', '96', '95', '84', '88', '97', '143', '81', '88', '79', '82', '94', '111', '77', '116', '58', '58', '87', '159', '58', '60', '49', '155', '53', '54', '48', '61', '46', '53', '69', '37', '75', '38', '56', '44', '35', '33', '34', '50', '23', '59', '31', '21', '36', '20', '24', '14', '28', '75', '17', '21', '19', '18', '12', '14', '18', '24', '43', '6', '19']
model: llama-30b, num requests: 60, total length: 60, prompt/kv_cache length: ['94', '97', '96', '85', '89', '98', '144', '82', '89', '80', '83', '95', '112', '78', '117', '59', '59', '88', '160', '59', '61', '50', '156', '54', '55', '49', '62', '47', '54', '70', '38', '76', '39', '57', '45', '36', '34', '35', '51', '24', '60', '32', '22', '37', '21', '25', '15', '29', '76', '18', '22', '20', '19', '13', '15', '19', '25', '44', '7', '20']
[9.5s] Avg Throughput: propmt: 226, generation: 578
model: llama-30b, num requests: 60, total length: 60, prompt/kv_cache length: ['95', '98', '97', '86', '90', '99', '145', '83', '90', '81', '84', '96', '113', '79', '118', '60', '60', '89', '161', '60', '62', '51', '157', '55', '56', '50', '63', '48', '55', '71', '39', '77', '40', '58', '46', '37', '35', '36', '52', '25', '61', '33', '23', '38', '22', '26', '16', '30', '77', '19', '23', '21', '20', '14', '16', '20', '26', '45', '8', '21']
model: llama-30b, num requests: 59, total length: 59, prompt/kv_cache length: ['96', '99', '98', '87', '91', '100', '146', '84', '91', '82', '85', '97', '114', '80', '119', '61', '61', '90', '162', '61', '63', '52', '158', '56', '57', '51', '64', '49', '56', '72', '40', '78', '41', '59', '47', '38', '36', '37', '53', '26', '62', '34', '24', '39', '23', '27', '17', '31', '78', '20', '24', '22', '21', '17', '21', '27', '46', '9', '22']
model: llama-30b, num requests: 58, total length: 58, prompt/kv_cache length: ['97', '99', '88', '92', '101', '147', '85', '92', '83', '86', '98', '115', '81', '120', '62', '62', '91', '163', '62', '64', '53', '159', '57', '58', '52', '65', '50', '57', '73', '41', '79', '42', '60', '48', '39', '37', '38', '54', '27', '63', '35', '25', '40', '24', '28', '18', '32', '79', '21', '25', '23', '22', '18', '22', '28', '47', '10', '23']
model: llama-30b, num requests: 58, total length: 58, prompt/kv_cache length: ['98', '100', '89', '93', '102', '148', '86', '93', '84', '87', '99', '116', '82', '121', '63', '63', '92', '164', '63', '65', '54', '160', '58', '59', '53', '66', '51', '58', '74', '42', '80', '43', '61', '49', '40', '38', '39', '55', '28', '64', '36', '26', '41', '25', '29', '19', '33', '80', '22', '26', '24', '23', '19', '23', '29', '48', '11', '24']
model: llama-30b, num requests: 56, total length: 56, prompt/kv_cache length: ['101', '90', '94', '103', '149', '87', '94', '85', '88', '100', '117', '83', '122', '64', '64', '93', '165', '64', '66', '55', '161', '59', '60', '54', '67', '52', '59', '75', '43', '81', '44', '62', '50', '41', '39', '40', '56', '29', '65', '37', '27', '42', '26', '30', '20', '34', '81', '27', '25', '24', '20', '24', '30', '49', '12', '25']
model: llama-30b, num requests: 55, total length: 55, prompt/kv_cache length: ['102', '91', '95', '104', '150', '88', '95', '86', '89', '101', '118', '123', '65', '65', '94', '166', '65', '67', '56', '162', '60', '61', '55', '68', '53', '60', '76', '44', '82', '45', '63', '51', '42', '40', '41', '57', '30', '66', '38', '28', '43', '27', '31', '21', '35', '82', '28', '26', '25', '21', '25', '31', '50', '13', '26']
[10.0s] Avg Throughput: propmt: 0, generation: 702
model: llama-30b, num requests: 55, total length: 55, prompt/kv_cache length: ['103', '92', '96', '105', '151', '89', '96', '87', '90', '102', '119', '124', '66', '66', '95', '167', '66', '68', '57', '163', '61', '62', '56', '69', '54', '61', '77', '45', '83', '46', '64', '52', '43', '41', '42', '58', '31', '67', '39', '29', '44', '28', '32', '22', '36', '83', '29', '27', '26', '22', '26', '32', '51', '14', '27']
model: llama-30b, num requests: 55, total length: 55, prompt/kv_cache length: ['104', '93', '97', '106', '152', '90', '97', '88', '91', '103', '120', '125', '67', '67', '96', '168', '67', '69', '58', '164', '62', '63', '57', '70', '55', '62', '78', '46', '84', '47', '65', '53', '44', '42', '43', '59', '32', '68', '40', '30', '45', '29', '33', '23', '37', '84', '30', '28', '27', '23', '27', '33', '52', '15', '28']
model: llama-30b, num requests: 55, total length: 55, prompt/kv_cache length: ['105', '94', '98', '107', '153', '91', '98', '89', '92', '104', '121', '126', '68', '68', '97', '169', '68', '70', '59', '165', '63', '64', '58', '71', '56', '63', '79', '47', '85', '48', '66', '54', '45', '43', '44', '60', '33', '69', '41', '31', '46', '30', '34', '24', '38', '85', '31', '29', '28', '24', '28', '34', '53', '16', '29']
model: llama-30b, num requests: 54, total length: 54, prompt/kv_cache length: ['106', '95', '99', '108', '154', '92', '99', '90', '93', '105', '122', '127', '69', '69', '98', '170', '69', '71', '60', '166', '64', '65', '59', '72', '57', '64', '80', '48', '86', '49', '67', '55', '46', '44', '45', '61', '34', '70', '42', '32', '47', '31', '35', '39', '86', '32', '30', '29', '25', '29', '35', '54', '17', '30']
model: llama-30b, num requests: 53, total length: 53, prompt/kv_cache length: ['107', '96', '100', '109', '155', '93', '100', '94', '106', '123', '128', '70', '70', '99', '171', '70', '72', '61', '167', '65', '66', '60', '73', '58', '65', '81', '49', '87', '50', '68', '56', '47', '45', '46', '62', '35', '71', '43', '33', '48', '32', '36', '40', '87', '33', '31', '30', '26', '30', '36', '55', '18', '31']
model: llama-30b, num requests: 52, total length: 52, prompt/kv_cache length: ['108', '97', '101', '110', '156', '94', '101', '95', '107', '124', '129', '71', '100', '172', '71', '73', '62', '168', '66', '67', '61', '74', '59', '66', '82', '50', '88', '51', '69', '57', '48', '46', '47', '63', '36', '72', '44', '34', '49', '33', '37', '41', '88', '34', '32', '31', '27', '31', '37', '56', '19', '32']
model: llama-30b, num requests: 51, total length: 51, prompt/kv_cache length: ['109', '98', '102', '111', '157', '102', '96', '108', '125', '130', '72', '101', '173', '72', '74', '63', '169', '67', '68', '62', '75', '60', '67', '83', '51', '89', '52', '70', '58', '49', '47', '48', '64', '37', '73', '45', '35', '50', '34', '38', '42', '89', '35', '33', '32', '28', '32', '38', '57', '20', '33']
[10.5s] Avg Throughput: propmt: 0, generation: 758
model: llama-30b, num requests: 51, total length: 51, prompt/kv_cache length: ['110', '99', '103', '112', '158', '103', '97', '109', '126', '131', '73', '102', '174', '73', '75', '64', '170', '68', '69', '63', '76', '61', '68', '84', '52', '90', '53', '71', '59', '50', '48', '49', '65', '38', '74', '46', '36', '51', '35', '39', '43', '90', '36', '34', '33', '29', '33', '39', '58', '21', '34']
model: llama-30b, num requests: 51, total length: 51, prompt/kv_cache length: ['111', '100', '104', '113', '159', '104', '98', '110', '127', '132', '74', '103', '175', '74', '76', '65', '171', '69', '70', '64', '77', '62', '69', '85', '53', '91', '54', '72', '60', '51', '49', '50', '66', '39', '75', '47', '37', '52', '36', '40', '44', '91', '37', '35', '34', '30', '34', '40', '59', '22', '35']
model: llama-30b, num requests: 51, total length: 51, prompt/kv_cache length: ['112', '101', '105', '114', '160', '105', '99', '111', '128', '133', '75', '104', '176', '75', '77', '66', '172', '70', '71', '65', '78', '63', '70', '86', '54', '92', '55', '73', '61', '52', '50', '51', '67', '40', '76', '48', '38', '53', '37', '41', '45', '92', '38', '36', '35', '31', '35', '41', '60', '23', '36']
model: llama-30b, num requests: 50, total length: 50, prompt/kv_cache length: ['102', '106', '115', '161', '106', '100', '112', '129', '134', '76', '105', '177', '76', '78', '67', '173', '71', '72', '66', '79', '64', '71', '87', '55', '93', '56', '74', '62', '53', '51', '52', '68', '41', '77', '49', '39', '54', '38', '42', '46', '93', '39', '37', '36', '32', '36', '42', '61', '24', '37']
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['103', '107', '116', '162', '107', '101', '113', '130', '135', '77', '106', '178', '77', '79', '68', '174', '72', '73', '67', '80', '65', '72', '88', '56', '94', '57', '75', '63', '54', '52', '53', '69', '42', '78', '50', '40', '55', '39', '43', '47', '40', '38', '37', '33', '37', '43', '62', '25', '38']
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['104', '108', '117', '163', '108', '102', '114', '131', '136', '78', '107', '179', '78', '80', '69', '175', '73', '74', '68', '81', '66', '73', '89', '57', '95', '58', '76', '64', '55', '53', '54', '70', '43', '79', '51', '41', '56', '40', '44', '48', '41', '39', '38', '34', '38', '44', '63', '26', '39']
[11.0s] Avg Throughput: propmt: 0, generation: 606
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['105', '109', '118', '164', '109', '103', '115', '132', '137', '79', '108', '180', '79', '81', '70', '176', '74', '75', '69', '82', '67', '74', '90', '58', '96', '59', '77', '65', '56', '54', '55', '71', '44', '80', '52', '42', '57', '41', '45', '49', '42', '40', '39', '35', '39', '45', '64', '27', '40']
model: llama-30b, num requests: 49, total length: 49, prompt/kv_cache length: ['106', '110', '119', '165', '110', '104', '116', '133', '138', '80', '109', '181', '80', '82', '71', '177', '75', '76', '70', '83', '68', '75', '91', '59', '97', '60', '78', '66', '57', '55', '56', '72', '45', '81', '53', '43', '58', '42', '46', '50', '43', '41', '40', '36', '40', '46', '65', '28', '41']
model: llama-30b, num requests: 48, total length: 48, prompt/kv_cache length: ['111', '120', '166', '111', '105', '117', '134', '139', '81', '110', '182', '81', '83', '72', '178', '76', '77', '71', '84', '69', '76', '92', '60', '98', '61', '79', '67', '58', '56', '57', '73', '46', '82', '54', '44', '59', '43', '47', '51', '44', '42', '41', '37', '41', '47', '66', '29', '42']
model: llama-30b, num requests: 48, total length: 48, prompt/kv_cache length: ['112', '121', '167', '112', '106', '118', '135', '140', '82', '111', '183', '82', '84', '73', '179', '77', '78', '72', '85', '70', '77', '93', '61', '99', '62', '80', '68', '59', '57', '58', '74', '47', '83', '55', '45', '60', '44', '48', '52', '45', '43', '42', '38', '42', '48', '67', '30', '43']
model: llama-30b, num requests: 47, total length: 47, prompt/kv_cache length: ['113', '122', '168', '113', '107', '119', '136', '141', '83', '112', '184', '83', '74', '180', '78', '79', '73', '86', '71', '78', '94', '62', '100', '63', '81', '69', '60', '58', '59', '75', '48', '84', '56', '46', '61', '45', '49', '53', '46', '44', '43', '39', '43', '49', '68', '31', '44']
model: llama-30b, num requests: 47, total length: 47, prompt/kv_cache length: ['114', '123', '169', '114', '108', '120', '137', '142', '84', '113', '185', '84', '75', '181', '79', '80', '74', '87', '72', '79', '95', '63', '101', '64', '82', '70', '61', '59', '60', '76', '49', '85', '57', '47', '62', '46', '50', '54', '47', '45', '44', '40', '44', '50', '69', '32', '45']
model: llama-30b, num requests: 46, total length: 46, prompt/kv_cache length: ['115', '124', '170', '115', '109', '121', '138', '143', '85', '114', '186', '85', '76', '182', '80', '81', '75', '88', '73', '96', '64', '102', '65', '83', '71', '62', '60', '61', '77', '50', '86', '58', '48', '63', '47', '51', '55', '48', '46', '45', '41', '45', '51', '70', '33', '46']
[11.5s] Avg Throughput: propmt: 0, generation: 674
model: llama-30b, num requests: 46, total length: 46, prompt/kv_cache length: ['116', '125', '171', '116', '110', '122', '139', '144', '86', '115', '187', '86', '77', '183', '81', '82', '76', '89', '74', '97', '65', '103', '66', '84', '72', '63', '61', '62', '78', '51', '87', '59', '49', '64', '48', '52', '56', '49', '47', '46', '42', '46', '52', '71', '34', '47']
model: llama-30b, num requests: 44, total length: 44, prompt/kv_cache length: ['117', '126', '172', '111', '123', '140', '145', '87', '116', '188', '87', '78', '184', '82', '83', '77', '90', '75', '98', '104', '67', '85', '73', '64', '62', '63', '79', '52', '88', '60', '50', '65', '49', '53', '57', '50', '48', '47', '43', '47', '53', '72', '35', '48']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['118', '127', '173', '112', '124', '141', '146', '88', '117', '189', '88', '79', '185', '83', '84', '78', '91', '76', '99', '68', '86', '74', '65', '63', '64', '80', '53', '89', '61', '51', '66', '50', '54', '58', '51', '49', '48', '44', '48', '54', '73', '36', '49']
model: llama-30b, num requests: 43, total length: 43, prompt/kv_cache length: ['119', '128', '174', '113', '125', '142', '147', '89', '118', '190', '89', '80', '186', '84', '85', '79', '92', '77', '100', '69', '87', '75', '66', '64', '65', '81', '54', '90', '62', '52', '67', '51', '55', '59', '52', '50', '49', '45', '49', '55', '74', '37', '50']
model: llama-30b, num requests: 42, total length: 42, prompt/kv_cache length: ['120', '129', '175', '114', '126', '143', '148', '90', '119', '191', '81', '187', '85', '86', '80', '93', '78', '101', '70', '88', '76', '67', '65', '66', '82', '55', '91', '63', '53', '68', '52', '56', '60', '53', '51', '50', '46', '50', '56', '75', '38', '51']
model: llama-30b, num requests: 42, total length: 42, prompt/kv_cache length: ['121', '130', '176', '115', '127', '144', '149', '91', '120', '192', '82', '188', '86', '87', '81', '94', '79', '102', '71', '89', '77', '68', '66', '67', '83', '56', '92', '64', '54', '69', '53', '57', '61', '54', '52', '51', '47', '51', '57', '76', '39', '52']
model: llama-30b, num requests: 41, total length: 41, prompt/kv_cache length: ['122', '131', '177', '116', '128', '145', '92', '121', '193', '83', '189', '87', '88', '82', '95', '80', '103', '72', '90', '78', '69', '67', '68', '84', '57', '93', '65', '55', '70', '54', '58', '62', '55', '53', '52', '48', '52', '58', '77', '40', '53']
[12.0s] Avg Throughput: propmt: 0, generation: 612
model: llama-30b, num requests: 40, total length: 40, prompt/kv_cache length: ['123', '132', '178', '117', '146', '93', '122', '194', '84', '190', '88', '89', '83', '96', '81', '104', '73', '91', '79', '70', '68', '69', '85', '58', '94', '66', '56', '71', '55', '59', '63', '56', '54', '53', '49', '53', '59', '78', '41', '54']
model: llama-30b, num requests: 40, total length: 40, prompt/kv_cache length: ['124', '133', '179', '118', '147', '94', '123', '195', '85', '191', '89', '90', '84', '97', '82', '105', '74', '92', '80', '71', '69', '70', '86', '59', '95', '67', '57', '72', '56', '60', '64', '57', '55', '54', '50', '54', '60', '79', '42', '55']
model: llama-30b, num requests: 40, total length: 40, prompt/kv_cache length: ['125', '134', '180', '119', '148', '95', '124', '196', '86', '192', '90', '91', '85', '98', '83', '106', '75', '93', '81', '72', '70', '71', '87', '60', '96', '68', '58', '73', '57', '61', '65', '58', '56', '55', '51', '55', '61', '80', '43', '56']
model: llama-30b, num requests: 38, total length: 38, prompt/kv_cache length: ['126', '135', '120', '149', '96', '125', '197', '87', '193', '91', '86', '99', '84', '107', '76', '94', '82', '73', '71', '72', '88', '61', '97', '69', '59', '74', '58', '62', '66', '59', '57', '56', '52', '56', '62', '81', '44', '57']
model: llama-30b, num requests: 36, total length: 36, prompt/kv_cache length: ['136', '121', '150', '97', '126', '198', '88', '194', '92', '100', '85', '108', '77', '95', '83', '74', '72', '73', '89', '62', '98', '70', '60', '75', '59', '63', '67', '60', '58', '57', '53', '57', '63', '82', '45', '58']
model: llama-30b, num requests: 36, total length: 36, prompt/kv_cache length: ['137', '122', '151', '98', '127', '199', '89', '195', '93', '101', '86', '109', '78', '96', '84', '75', '73', '74', '90', '63', '99', '71', '61', '76', '60', '64', '68', '61', '59', '58', '54', '58', '64', '83', '46', '59']
model: llama-30b, num requests: 36, total length: 36, prompt/kv_cache length: ['138', '123', '152', '99', '128', '200', '90', '196', '94', '102', '87', '110', '79', '97', '85', '76', '74', '75', '91', '64', '100', '72', '62', '77', '61', '65', '69', '62', '60', '59', '55', '59', '65', '84', '47', '60']
[12.5s] Avg Throughput: propmt: 0, generation: 542
model: llama-30b, num requests: 35, total length: 35, prompt/kv_cache length: ['139', '124', '153', '100', '129', '201', '91', '197', '95', '103', '88', '111', '80', '86', '77', '75', '76', '92', '65', '101', '73', '63', '78', '62', '66', '70', '63', '61', '60', '56', '60', '66', '85', '48', '61']
model: llama-30b, num requests: 34, total length: 34, prompt/kv_cache length: ['125', '154', '101', '130', '202', '92', '198', '96', '104', '89', '112', '81', '87', '78', '76', '77', '93', '66', '102', '74', '64', '79', '63', '67', '71', '64', '62', '61', '57', '61', '67', '86', '49', '62']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['126', '102', '131', '203', '199', '97', '105', '90', '113', '82', '88', '79', '77', '78', '94', '67', '103', '75', '65', '80', '64', '68', '72', '65', '63', '62', '58', '62', '68', '87', '50', '63']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['127', '103', '132', '204', '200', '98', '106', '91', '114', '83', '89', '80', '78', '79', '95', '68', '104', '76', '66', '81', '65', '69', '73', '66', '64', '63', '59', '63', '69', '88', '51', '64']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['128', '104', '133', '205', '201', '99', '107', '92', '115', '84', '90', '81', '79', '80', '96', '69', '105', '77', '67', '82', '66', '70', '74', '67', '65', '64', '60', '64', '70', '89', '52', '65']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['129', '105', '134', '206', '202', '100', '108', '93', '116', '85', '91', '82', '80', '81', '97', '70', '106', '78', '68', '83', '67', '71', '75', '68', '66', '65', '61', '65', '71', '90', '53', '66']
model: llama-30b, num requests: 32, total length: 32, prompt/kv_cache length: ['130', '106', '135', '207', '203', '101', '109', '94', '117', '86', '92', '83', '81', '82', '98', '71', '107', '79', '69', '84', '68', '72', '76', '69', '67', '66', '62', '66', '72', '91', '54', '67']
model: llama-30b, num requests: 30, total length: 30, prompt/kv_cache length: ['107', '136', '208', '204', '102', '110', '95', '118', '87', '93', '84', '82', '83', '99', '72', '108', '80', '70', '85', '69', '73', '70', '68', '67', '63', '67', '73', '92', '55', '68']
[13.0s] Avg Throughput: propmt: 0, generation: 530
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['108', '137', '209', '205', '103', '111', '96', '119', '88', '85', '83', '84', '100', '73', '109', '81', '71', '86', '70', '74', '71', '69', '68', '64', '68', '74', '93', '56', '69']
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['109', '138', '210', '206', '104', '112', '97', '120', '89', '86', '84', '85', '101', '74', '110', '82', '72', '87', '71', '75', '72', '70', '69', '65', '69', '75', '94', '57', '70']
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['110', '139', '211', '207', '105', '113', '98', '121', '90', '87', '85', '86', '102', '75', '111', '83', '73', '88', '72', '76', '73', '71', '70', '66', '70', '76', '95', '58', '71']
model: llama-30b, num requests: 29, total length: 29, prompt/kv_cache length: ['111', '140', '212', '208', '106', '114', '99', '122', '91', '88', '86', '87', '103', '76', '112', '84', '74', '89', '73', '77', '74', '72', '71', '67', '71', '77', '96', '59', '72']
model: llama-30b, num requests: 27, total length: 27, prompt/kv_cache length: ['112', '141', '209', '107', '115', '100', '123', '92', '89', '88', '104', '77', '113', '85', '75', '90', '74', '78', '75', '73', '72', '68', '72', '78', '97', '60', '73']
model: llama-30b, num requests: 27, total length: 27, prompt/kv_cache length: ['113', '142', '210', '108', '116', '101', '124', '93', '90', '89', '105', '78', '114', '86', '76', '91', '75', '79', '76', '74', '73', '69', '73', '79', '98', '61', '74']
model: llama-30b, num requests: 27, total length: 27, prompt/kv_cache length: ['114', '143', '211', '109', '117', '102', '125', '94', '91', '90', '106', '79', '115', '87', '77', '92', '76', '80', '77', '75', '74', '70', '74', '80', '99', '62', '75']
model: llama-30b, num requests: 27, total length: 27, prompt/kv_cache length: ['115', '144', '212', '110', '118', '103', '126', '95', '92', '91', '107', '80', '116', '88', '78', '93', '77', '81', '78', '76', '75', '71', '75', '81', '100', '63', '76']
[13.5s] Avg Throughput: propmt: 0, generation: 454
model: llama-30b, num requests: 26, total length: 26, prompt/kv_cache length: ['116', '145', '213', '111', '119', '104', '96', '93', '92', '108', '81', '117', '89', '79', '94', '78', '82', '79', '77', '76', '72', '76', '82', '101', '64', '77']
model: llama-30b, num requests: 26, total length: 26, prompt/kv_cache length: ['117', '146', '214', '112', '120', '105', '97', '94', '93', '109', '82', '118', '90', '80', '95', '79', '83', '80', '78', '77', '73', '77', '83', '102', '65', '78']
model: llama-30b, num requests: 26, total length: 26, prompt/kv_cache length: ['118', '147', '215', '113', '121', '106', '98', '95', '94', '110', '83', '119', '91', '81', '96', '80', '84', '81', '79', '78', '74', '78', '84', '103', '66', '79']
model: llama-30b, num requests: 25, total length: 25, prompt/kv_cache length: ['119', '148', '216', '114', '122', '107', '99', '96', '111', '84', '120', '92', '82', '97', '81', '85', '82', '80', '79', '75', '79', '85', '104', '67', '80']
model: llama-30b, num requests: 25, total length: 25, prompt/kv_cache length: ['120', '149', '217', '115', '123', '108', '100', '97', '112', '85', '121', '93', '83', '98', '82', '86', '83', '81', '80', '76', '80', '86', '105', '68', '81']
model: llama-30b, num requests: 25, total length: 25, prompt/kv_cache length: ['121', '150', '218', '116', '124', '109', '101', '98', '113', '86', '122', '94', '84', '99', '83', '87', '84', '82', '81', '77', '81', '87', '106', '69', '82']
model: llama-30b, num requests: 25, total length: 25, prompt/kv_cache length: ['122', '151', '219', '117', '125', '110', '102', '99', '114', '87', '123', '95', '85', '100', '84', '88', '85', '83', '82', '78', '82', '88', '107', '70', '83']
model: llama-30b, num requests: 25, total length: 25, prompt/kv_cache length: ['123', '152', '220', '118', '126', '111', '103', '100', '115', '88', '124', '96', '86', '101', '85', '89', '86', '84', '83', '79', '83', '89', '108', '71', '84']
model: llama-30b, num requests: 24, total length: 24, prompt/kv_cache length: ['153', '221', '119', '127', '112', '104', '101', '116', '89', '125', '97', '87', '102', '86', '90', '87', '85', '84', '80', '84', '90', '109', '72', '85']
[14.0s] Avg Throughput: propmt: 0, generation: 460
model: llama-30b, num requests: 24, total length: 24, prompt/kv_cache length: ['154', '222', '120', '128', '113', '105', '102', '117', '90', '126', '98', '88', '103', '87', '91', '88', '86', '85', '81', '85', '91', '110', '73', '86']
model: llama-30b, num requests: 23, total length: 23, prompt/kv_cache length: ['155', '223', '121', '129', '114', '106', '103', '118', '91', '127', '99', '89', '104', '88', '92', '89', '87', '86', '82', '86', '92', '111', '87']
model: llama-30b, num requests: 22, total length: 22, prompt/kv_cache length: ['156', '224', '130', '115', '107', '104', '119', '92', '128', '100', '90', '105', '89', '93', '90', '88', '87', '83', '87', '93', '112', '88']
model: llama-30b, num requests: 22, total length: 22, prompt/kv_cache length: ['157', '225', '131', '116', '108', '105', '120', '93', '129', '101', '91', '106', '90', '94', '91', '89', '88', '84', '88', '94', '113', '89']
model: llama-30b, num requests: 22, total length: 22, prompt/kv_cache length: ['158', '226', '132', '117', '109', '106', '121', '94', '130', '102', '92', '107', '91', '95', '92', '90', '89', '85', '89', '95', '114', '90']
model: llama-30b, num requests: 22, total length: 22, prompt/kv_cache length: ['159', '227', '133', '118', '110', '107', '122', '95', '131', '103', '93', '108', '92', '96', '93', '91', '90', '86', '90', '96', '115', '91']
model: llama-30b, num requests: 21, total length: 21, prompt/kv_cache length: ['160', '228', '134', '119', '111', '108', '123', '132', '104', '94', '109', '93', '97', '94', '92', '91', '87', '91', '97', '116', '92']
model: llama-30b, num requests: 19, total length: 19, prompt/kv_cache length: ['161', '229', '112', '109', '124', '133', '105', '95', '110', '94', '98', '95', '93', '92', '88', '92', '98', '117', '93']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['230', '113', '110', '125', '134', '106', '96', '95', '99', '96', '94', '93', '89', '93', '99', '118', '94']
[14.5s] Avg Throughput: propmt: 0, generation: 398
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['231', '114', '111', '126', '135', '107', '97', '96', '100', '97', '95', '94', '90', '94', '100', '119', '95']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['232', '115', '112', '127', '136', '108', '98', '97', '101', '98', '96', '95', '91', '95', '101', '120', '96']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['233', '116', '113', '128', '137', '109', '99', '98', '102', '99', '97', '96', '92', '96', '102', '121', '97']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['234', '117', '114', '129', '138', '110', '100', '99', '103', '100', '98', '97', '93', '97', '103', '122', '98']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['235', '118', '115', '130', '139', '111', '101', '100', '104', '101', '99', '98', '94', '98', '104', '123', '99']
model: llama-30b, num requests: 17, total length: 17, prompt/kv_cache length: ['236', '119', '116', '131', '140', '112', '102', '101', '105', '102', '100', '99', '95', '99', '105', '124', '100']
model: llama-30b, num requests: 16, total length: 16, prompt/kv_cache length: ['237', '120', '117', '132', '141', '113', '103', '102', '106', '103', '101', '100', '96', '100', '125', '101']
model: llama-30b, num requests: 15, total length: 15, prompt/kv_cache length: ['121', '118', '133', '142', '114', '104', '103', '107', '104', '102', '101', '97', '101', '126', '102']
model: llama-30b, num requests: 14, total length: 14, prompt/kv_cache length: ['122', '119', '134', '143', '105', '104', '108', '105', '103', '102', '98', '102', '127', '103']
model: llama-30b, num requests: 14, total length: 14, prompt/kv_cache length: ['123', '120', '135', '144', '106', '105', '109', '106', '104', '103', '99', '103', '128', '104']
[15.0s] Avg Throughput: propmt: 0, generation: 328
model: llama-30b, num requests: 13, total length: 13, prompt/kv_cache length: ['124', '121', '136', '145', '107', '110', '107', '105', '104', '100', '104', '129', '105']
model: llama-30b, num requests: 13, total length: 13, prompt/kv_cache length: ['125', '122', '137', '146', '108', '111', '108', '106', '105', '101', '105', '130', '106']
model: llama-30b, num requests: 13, total length: 13, prompt/kv_cache length: ['126', '123', '138', '147', '109', '112', '109', '107', '106', '102', '106', '131', '107']
model: llama-30b, num requests: 12, total length: 12, prompt/kv_cache length: ['127', '124', '139', '148', '110', '113', '110', '107', '103', '107', '132', '108']
model: llama-30b, num requests: 12, total length: 12, prompt/kv_cache length: ['128', '125', '140', '149', '111', '114', '111', '108', '104', '108', '133', '109']
model: llama-30b, num requests: 12, total length: 12, prompt/kv_cache length: ['129', '126', '141', '150', '112', '115', '112', '109', '105', '109', '134', '110']
model: llama-30b, num requests: 11, total length: 11, prompt/kv_cache length: ['130', '127', '151', '113', '116', '113', '110', '106', '110', '135', '111']
model: llama-30b, num requests: 11, total length: 11, prompt/kv_cache length: ['131', '128', '152', '114', '117', '114', '111', '107', '111', '136', '112']
model: llama-30b, num requests: 11, total length: 11, prompt/kv_cache length: ['132', '129', '153', '115', '118', '115', '112', '108', '112', '137', '113']
model: llama-30b, num requests: 10, total length: 10, prompt/kv_cache length: ['133', '154', '116', '119', '116', '113', '109', '113', '138', '114']
[15.5s] Avg Throughput: propmt: 0, generation: 244
model: llama-30b, num requests: 9, total length: 9, prompt/kv_cache length: ['134', '155', '117', '117', '114', '110', '114', '139', '115']
model: llama-30b, num requests: 9, total length: 9, prompt/kv_cache length: ['135', '156', '118', '118', '115', '111', '115', '140', '116']
model: llama-30b, num requests: 9, total length: 9, prompt/kv_cache length: ['136', '157', '119', '119', '116', '112', '116', '141', '117']
model: llama-30b, num requests: 7, total length: 7, prompt/kv_cache length: ['137', '158', '120', '120', '117', '142', '118']
model: llama-30b, num requests: 6, total length: 6, prompt/kv_cache length: ['138', '159', '121', '121', '143', '119']
model: llama-30b, num requests: 6, total length: 6, prompt/kv_cache length: ['139', '160', '122', '122', '144', '120']
model: llama-30b, num requests: 4, total length: 4, prompt/kv_cache length: ['161', '123', '145', '121']
model: llama-30b, num requests: 4, total length: 4, prompt/kv_cache length: ['162', '124', '146', '122']
model: llama-30b, num requests: 4, total length: 4, prompt/kv_cache length: ['163', '125', '147', '123']
model: llama-30b, num requests: 3, total length: 3, prompt/kv_cache length: ['126', '148', '124']
model: llama-30b, num requests: 3, total length: 3, prompt/kv_cache length: ['127', '149', '125']
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['128', '150']
[16.0s] Avg Throughput: propmt: 0, generation: 148
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['129', '151']
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['130', '152']
model: llama-30b, num requests: 2, total length: 2, prompt/kv_cache length: ['131', '153']
model: llama-30b, num requests: 1, total length: 1, prompt/kv_cache length: ['154']
[16.5s] Avg Throughput: propmt: 0, generation: 18
---------------------------
Exiting The Simulator
Memory Is All Freed
Checking Non-Exited Systems ...
---------------------------
All Request Has Been Exited
---------------------------
{'id': 3, 'model': 'llama-30b', 'input': 4, 'output': 16, 'arrival': 570907776, 'end_time': 1327076367, 'latency': 756168591}
{'id': 7, 'model': 'llama-30b', 'input': 2, 'output': 11, 'arrival': 811936856, 'end_time': 1418068795, 'latency': 606131939}
{'id': 21, 'model': 'llama-30b', 'input': 10, 'output': 20, 'arrival': 1933551216, 'end_time': 2791186092, 'latency': 857634876}
{'id': 9, 'model': 'llama-30b', 'input': 5, 'output': 31, 'arrival': 1026970077, 'end_time': 2965341217, 'latency': 1938371140}
{'id': 25, 'model': 'llama-30b', 'input': 10, 'output': 20, 'arrival': 2228417559, 'end_time': 3087119819, 'latency': 858702260}
{'id': 18, 'model': 'llama-30b', 'input': 30, 'output': 51, 'arrival': 1789468376, 'end_time': 3412389067, 'latency': 1622920691}
{'id': 33, 'model': 'llama-30b', 'input': 43, 'output': 56, 'arrival': 2833725517, 'end_time': 4027784685, 'latency': 1194059168}
{'id': 40, 'model': 'llama-30b', 'input': 103, 'output': 105, 'arrival': 3568927610, 'end_time': 4027784685, 'latency': 458857075}
{'id': 20, 'model': 'llama-30b', 'input': 8, 'output': 37, 'arrival': 1918527763, 'end_time': 4790857005, 'latency': 2872329242}
{'id': 0, 'model': 'llama-30b', 'input': 10, 'output': 80, 'arrival': 0, 'end_time': 5093990005, 'latency': 5093990005}
{'id': 31, 'model': 'llama-30b', 'input': 8, 'output': 35, 'arrival': 2529630345, 'end_time': 5236486687, 'latency': 2706856342}
{'id': 28, 'model': 'llama-30b', 'input': 47, 'output': 78, 'arrival': 2412644533, 'end_time': 5503441030, 'latency': 3090796497}
{'id': 51, 'model': 'llama-30b', 'input': 4, 'output': 13, 'arrival': 4728227590, 'end_time': 5503441030, 'latency': 775213440}
{'id': 5, 'model': 'llama-30b', 'input': 15, 'output': 78, 'arrival': 604829893, 'end_time': 5872682795, 'latency': 5267852902}
{'id': 8, 'model': 'llama-30b', 'input': 18, 'output': 76, 'arrival': 903845071, 'end_time': 6141937066, 'latency': 5238091995}
{'id': 38, 'model': 'llama-30b', 'input': 2, 'output': 23, 'arrival': 3497903303, 'end_time': 6141937066, 'latency': 2644033763}
{'id': 39, 'model': 'llama-30b', 'input': 1, 'output': 21, 'arrival': 3555912387, 'end_time': 6141937066, 'latency': 2586024679}
{'id': 58, 'model': 'llama-30b', 'input': 24, 'output': 27, 'arrival': 5615708423, 'end_time': 6272174797, 'latency': 656466374}
{'id': 2, 'model': 'llama-30b', 'input': 16, 'output': 85, 'arrival': 479613521, 'end_time': 6378195914, 'latency': 5898582393}
{'id': 56, 'model': 'llama-30b', 'input': 91, 'output': 95, 'arrival': 5589266757, 'end_time': 6378195914, 'latency': 788929157}
{'id': 36, 'model': 'llama-30b', 'input': 1, 'output': 28, 'arrival': 3372350495, 'end_time': 6529955322, 'latency': 3157604827}
{'id': 10, 'model': 'llama-30b', 'input': 83, 'output': 147, 'arrival': 1029050008, 'end_time': 6837747930, 'latency': 5808697922}
{'id': 45, 'model': 'llama-30b', 'input': 20, 'output': 46, 'arrival': 4019395352, 'end_time': 6837747930, 'latency': 2818352578}
{'id': 22, 'model': 'llama-30b', 'input': 4, 'output': 54, 'arrival': 1968102767, 'end_time': 6956736491, 'latency': 4988633724}
{'id': 52, 'model': 'llama-30b', 'input': 16, 'output': 35, 'arrival': 5008737032, 'end_time': 6956736491, 'latency': 1947999459}
{'id': 19, 'model': 'llama-30b', 'input': 25, 'output': 79, 'arrival': 1823890676, 'end_time': 7146017121, 'latency': 5322126445}
{'id': 43, 'model': 'llama-30b', 'input': 17, 'output': 47, 'arrival': 3880823994, 'end_time': 7146017121, 'latency': 3265193127}
{'id': 73, 'model': 'llama-30b', 'input': 11, 'output': 12, 'arrival': 7025776321, 'end_time': 7146017121, 'latency': 120240800}
{'id': 16, 'model': 'llama-30b', 'input': 23, 'output': 83, 'arrival': 1658521887, 'end_time': 7396397723, 'latency': 5737875836}
{'id': 6, 'model': 'llama-30b', 'input': 15, 'output': 99, 'arrival': 610813769, 'end_time': 7989287597, 'latency': 7378473828}
{'id': 69, 'model': 'llama-30b', 'input': 7, 'output': 21, 'arrival': 6686130579, 'end_time': 8079024576, 'latency': 1392893997}
{'id': 35, 'model': 'llama-30b', 'input': 8, 'output': 55, 'arrival': 3336021709, 'end_time': 8258151178, 'latency': 4922129469}
{'id': 63, 'model': 'llama-30b', 'input': 6, 'output': 29, 'arrival': 5956509192, 'end_time': 8258151178, 'latency': 2301641986}
{'id': 49, 'model': 'llama-30b', 'input': 73, 'output': 112, 'arrival': 4229722339, 'end_time': 8338711099, 'latency': 4108988760}
{'id': 37, 'model': 'llama-30b', 'input': 9, 'output': 60, 'arrival': 3382628226, 'end_time': 8753232727, 'latency': 5370604501}
{'id': 55, 'model': 'llama-30b', 'input': 39, 'output': 70, 'arrival': 5580001211, 'end_time': 8845862071, 'latency': 3265860860}
{'id': 53, 'model': 'llama-30b', 'input': 49, 'output': 87, 'arrival': 5233952232, 'end_time': 9018209220, 'latency': 3784256988}
{'id': 57, 'model': 'llama-30b', 'input': 24, 'output': 57, 'arrival': 5611080226, 'end_time': 9018209220, 'latency': 3407128994}
{'id': 4, 'model': 'llama-30b', 'input': 13, 'output': 109, 'arrival': 587870263, 'end_time': 9117761572, 'latency': 8529891309}
{'id': 1, 'model': 'llama-30b', 'input': 22, 'output': 126, 'arrival': 347938952, 'end_time': 9350323030, 'latency': 9002384078}
{'id': 93, 'model': 'llama-30b', 'input': 7, 'output': 15, 'arrival': 8788269096, 'end_time': 9603228298, 'latency': 814959202}
{'id': 12, 'model': 'llama-30b', 'input': 14, 'output': 100, 'arrival': 1558048710, 'end_time': 9687067494, 'latency': 8129018784}
{'id': 11, 'model': 'llama-30b', 'input': 8, 'output': 99, 'arrival': 1379405756, 'end_time': 9852649680, 'latency': 8473243924}
{'id': 89, 'model': 'llama-30b', 'input': 9, 'output': 23, 'arrival': 8425191197, 'end_time': 9852649680, 'latency': 1427458483}
{'id': 34, 'model': 'llama-30b', 'input': 17, 'output': 84, 'arrival': 3170788552, 'end_time': 9933912661, 'latency': 6763124109}
{'id': 86, 'model': 'llama-30b', 'input': 1, 'output': 25, 'arrival': 8041570457, 'end_time': 10256628415, 'latency': 2215057958}
{'id': 27, 'model': 'llama-30b', 'input': 8, 'output': 91, 'arrival': 2322894061, 'end_time': 10336346977, 'latency': 8013452916}
{'id': 44, 'model': 'llama-30b', 'input': 6, 'output': 71, 'arrival': 3910769772, 'end_time': 10415614841, 'latency': 6504845069}
{'id': 24, 'model': 'llama-30b', 'input': 7, 'output': 95, 'arrival': 2074623958, 'end_time': 10493788655, 'latency': 8419164697}
{'id': 13, 'model': 'llama-30b', 'input': 14, 'output': 113, 'arrival': 1581917473, 'end_time': 10804725759, 'latency': 9222808286}
{'id': 88, 'model': 'llama-30b', 'input': 66, 'output': 94, 'arrival': 8361284584, 'end_time': 10881420856, 'latency': 2520136272}
{'id': 14, 'model': 'llama-30b', 'input': 3, 'output': 107, 'arrival': 1601985371, 'end_time': 11186009622, 'latency': 9584024251}
{'id': 50, 'model': 'llama-30b', 'input': 12, 'output': 85, 'arrival': 4579003052, 'end_time': 11336413340, 'latency': 6757410288}
{'id': 66, 'model': 'llama-30b', 'input': 19, 'output': 80, 'arrival': 6082919999, 'end_time': 11485783632, 'latency': 5402863633}
{'id': 26, 'model': 'llama-30b', 'input': 16, 'output': 117, 'arrival': 2250691145, 'end_time': 11633105266, 'latency': 9382414121}
{'id': 68, 'model': 'llama-30b', 'input': 4, 'output': 66, 'arrival': 6252715945, 'end_time': 11633105266, 'latency': 5380389321}
{'id': 70, 'model': 'llama-30b', 'input': 49, 'output': 105, 'arrival': 6834078957, 'end_time': 11705236860, 'latency': 4871157903}
{'id': 48, 'model': 'llama-30b', 'input': 9, 'output': 90, 'arrival': 4209283479, 'end_time': 11848530402, 'latency': 7639246923}
{'id': 41, 'model': 'llama-30b', 'input': 63, 'output': 150, 'arrival': 3637282333, 'end_time': 11989728936, 'latency': 8352446603}
{'id': 30, 'model': 'llama-30b', 'input': 26, 'output': 129, 'arrival': 2510934220, 'end_time': 12059857224, 'latency': 9548923004}
{'id': 23, 'model': 'llama-30b', 'input': 68, 'output': 181, 'arrival': 2013730489, 'end_time': 12266986971, 'latency': 10253256482}
{'id': 61, 'model': 'llama-30b', 'input': 19, 'output': 92, 'arrival': 5735930705, 'end_time': 12266986971, 'latency': 6531056266}
{'id': 15, 'model': 'llama-30b', 'input': 8, 'output': 127, 'arrival': 1622246514, 'end_time': 12334462992, 'latency': 10712216478}
{'id': 62, 'model': 'llama-30b', 'input': 14, 'output': 87, 'arrival': 5912386492, 'end_time': 12334462992, 'latency': 6422076500}
{'id': 72, 'model': 'llama-30b', 'input': 31, 'output': 98, 'arrival': 6856786643, 'end_time': 12532259364, 'latency': 5675472721}
{'id': 17, 'model': 'llama-30b', 'input': 18, 'output': 140, 'arrival': 1732914670, 'end_time': 12597636379, 'latency': 10864721709}
{'id': 32, 'model': 'llama-30b', 'input': 43, 'output': 155, 'arrival': 2536356738, 'end_time': 12662035505, 'latency': 10125678767}
{'id': 54, 'model': 'llama-30b', 'input': 9, 'output': 93, 'arrival': 5325057673, 'end_time': 12662035505, 'latency': 7336977832}
{'id': 29, 'model': 'llama-30b', 'input': 13, 'output': 131, 'arrival': 2417400918, 'end_time': 12976151520, 'latency': 10558750602}
{'id': 87, 'model': 'llama-30b', 'input': 16, 'output': 77, 'arrival': 8143059393, 'end_time': 12976151520, 'latency': 4833092127}
{'id': 74, 'model': 'llama-30b', 'input': 22, 'output': 94, 'arrival': 7148485911, 'end_time': 13037428511, 'latency': 5888942600}
{'id': 47, 'model': 'llama-30b', 'input': 109, 'output': 213, 'arrival': 4130161099, 'end_time': 13280461459, 'latency': 9150300360}
{'id': 76, 'model': 'llama-30b', 'input': 14, 'output': 87, 'arrival': 7426573655, 'end_time': 13280461459, 'latency': 5853887804}
{'id': 67, 'model': 'llama-30b', 'input': 36, 'output': 127, 'arrival': 6244968358, 'end_time': 13517405925, 'latency': 7272437567}
{'id': 77, 'model': 'llama-30b', 'input': 15, 'output': 95, 'arrival': 7434266581, 'end_time': 13691874093, 'latency': 6257607512}
{'id': 42, 'model': 'llama-30b', 'input': 5, 'output': 124, 'arrival': 3640781705, 'end_time': 13980401028, 'latency': 10339619323}
{'id': 98, 'model': 'llama-30b', 'input': 5, 'output': 74, 'arrival': 9136063646, 'end_time': 14093704254, 'latency': 4957640608}
{'id': 60, 'model': 'llama-30b', 'input': 18, 'output': 122, 'arrival': 5704274661, 'end_time': 14149810323, 'latency': 8445535662}
{'id': 79, 'model': 'llama-30b', 'input': 5, 'output': 96, 'arrival': 7490970859, 'end_time': 14370148713, 'latency': 6879177854}
{'id': 64, 'model': 'llama-30b', 'input': 27, 'output': 135, 'arrival': 5989489476, 'end_time': 14424727705, 'latency': 8435238229}
{'id': 65, 'model': 'llama-30b', 'input': 12, 'output': 120, 'arrival': 6067730184, 'end_time': 14424727705, 'latency': 8356997521}
{'id': 46, 'model': 'llama-30b', 'input': 36, 'output': 162, 'arrival': 4056750010, 'end_time': 14477728109, 'latency': 10420978099}
{'id': 83, 'model': 'llama-30b', 'input': 21, 'output': 111, 'arrival': 7834202622, 'end_time': 14477728109, 'latency': 6643525487}
{'id': 96, 'model': 'llama-30b', 'input': 22, 'output': 106, 'arrival': 9077707450, 'end_time': 14838072741, 'latency': 5760365291}
{'id': 59, 'model': 'llama-30b', 'input': 120, 'output': 238, 'arrival': 5655061632, 'end_time': 14888510698, 'latency': 9233449066}
{'id': 81, 'model': 'llama-30b', 'input': 16, 'output': 115, 'arrival': 7787453935, 'end_time': 14938378257, 'latency': 7150924322}
{'id': 84, 'model': 'llama-30b', 'input': 5, 'output': 106, 'arrival': 7871451457, 'end_time': 15036006037, 'latency': 7164554580}
{'id': 91, 'model': 'llama-30b', 'input': 11, 'output': 108, 'arrival': 8562841071, 'end_time': 15180931630, 'latency': 6618090559}
{'id': 78, 'model': 'llama-30b', 'input': 32, 'output': 142, 'arrival': 7478655849, 'end_time': 15322719736, 'latency': 7844063887}
{'id': 75, 'model': 'llama-30b', 'input': 14, 'output': 130, 'arrival': 7279052202, 'end_time': 15463010548, 'latency': 8183958346}
{'id': 85, 'model': 'llama-30b', 'input': 10, 'output': 120, 'arrival': 7910782878, 'end_time': 15508699452, 'latency': 7597916574}
{'id': 92, 'model': 'llama-30b', 'input': 12, 'output': 117, 'arrival': 8705880346, 'end_time': 15644238180, 'latency': 6938357834}
{'id': 94, 'model': 'llama-30b', 'input': 11, 'output': 113, 'arrival': 8935658093, 'end_time': 15644238180, 'latency': 6708580087}
{'id': 95, 'model': 'llama-30b', 'input': 15, 'output': 118, 'arrival': 9003739566, 'end_time': 15687848988, 'latency': 6684109422}
{'id': 71, 'model': 'llama-30b', 'input': 13, 'output': 140, 'arrival': 6856232901, 'end_time': 15772956366, 'latency': 8916723465}
{'id': 90, 'model': 'llama-30b', 'input': 13, 'output': 123, 'arrival': 8437928437, 'end_time': 15772956366, 'latency': 7335027929}
{'id': 80, 'model': 'llama-30b', 'input': 43, 'output': 164, 'arrival': 7689823816, 'end_time': 15895926132, 'latency': 8206102316}
{'id': 99, 'model': 'llama-30b', 'input': 18, 'output': 126, 'arrival': 9147480390, 'end_time': 15976843176, 'latency': 6829362786}
{'id': 82, 'model': 'llama-30b', 'input': 6, 'output': 132, 'arrival': 7827635815, 'end_time': 16134428632, 'latency': 8306792817}
{'id': 97, 'model': 'llama-30b', 'input': 42, 'output': 155, 'arrival': 9133488869, 'end_time': 16173325187, 'latency': 7039836318}
---------------------------
Throughput Results
---------------------------
Total prompts: 2241 tokens/s
Total Generation: 6706 tokens/s
Throughput per 0.5 sec: [(64, 26), (166, 112), (192, 138), (270, 204), (262, 314), (360, 290), (70, 474), (212, 194), (294, 218), (414, 412), (148, 514), (356, 340), (550, 376), (112, 576), (182, 560), (314, 500), (54, 550), (236, 562), (226, 578), (0, 702), (0, 758), (0, 606), (0, 674), (0, 612), (0, 542), (0, 530), (0, 454), (0, 460), (0, 398), (0, 328), (0, 244), (0, 148), (0, 18)]
Total clocks: 16173325187 ticks
Total latency: 16.173325187 s
Average throughput: prompt: 138.5614877638953 generation: 414.6333498191351
---------------------------
Simulation Time (ms)
---------------------------
Total execution engine time: 123051.5
Total graph time: 84940.495
Total astra time: 65175.237
Total scheduler time: 2379.492
Total simulation time: 275546.724
