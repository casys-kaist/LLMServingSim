model: llama-7b, num requests: 1, total length: 10, prompt/kv_cache length: ['10']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['11']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['12']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['13']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['14']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['15']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['16']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['17']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['18']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['19']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['20']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['21']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['22']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['23']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['24']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['25']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['26']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['27']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['28']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['29']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['30']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['31']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['32']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['33']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['34']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['35']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['36']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['37']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['38']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['39']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['40']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['41']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['42']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['43']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['44']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['45']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['46']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['47']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['48']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['49']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['50']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['51']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['52']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['53']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['54']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['55']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['56']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['57']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['58']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['59']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['60']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['61']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['62']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['63']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['64']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['65']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['66']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['67']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['68']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['69']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['70']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['71']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['72']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['73']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['74']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['75']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['76']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['77']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['78']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['79']
model: llama-7b, num requests: 1, total length: 22, prompt/kv_cache length: ['22']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['23']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['24']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['25']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['26']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['27']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['28']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['29']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['30']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['31']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['32']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['33']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['34']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['35']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['36']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['37']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['38']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['39']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['40']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['41']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['42']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['43']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['44']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['45']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['46']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['47']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['48']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['49']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['50']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['51']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['52']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['53']
model: llama-7b, num requests: 2, total length: 17, prompt/kv_cache length: ['54', '16']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['55', '17']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['56', '18']
[0.5s] Avg Throughput: propmt: 96, generation: 206
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['57', '19']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['58', '20']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['59', '21']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['60', '22']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['61', '23']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['62', '24']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['63', '25']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['64', '26']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['65', '27']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['66', '28']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['67', '29']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['68', '30']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['69', '31']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['70', '32']
model: llama-7b, num requests: 3, total length: 6, prompt/kv_cache length: ['71', '33', '4']
model: llama-7b, num requests: 3, total length: 3, prompt/kv_cache length: ['72', '34', '5']
model: llama-7b, num requests: 4, total length: 16, prompt/kv_cache length: ['73', '35', '6', '13']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['74', '36', '7', '14']
model: llama-7b, num requests: 5, total length: 19, prompt/kv_cache length: ['75', '37', '8', '15', '15']
model: llama-7b, num requests: 6, total length: 20, prompt/kv_cache length: ['76', '38', '9', '16', '16', '15']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['77', '39', '10', '17', '17', '16']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['78', '40', '11', '18', '18', '17']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['79', '41', '12', '19', '19', '18']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['80', '42', '13', '20', '20', '19']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['81', '43', '14', '21', '21', '20']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['82', '44', '15', '22', '22', '21']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['83', '45', '23', '23', '22']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['84', '46', '24', '24', '23']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['85', '47', '25', '25', '24']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['86', '48', '26', '26', '25']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['87', '49', '27', '27', '26']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['88', '50', '28', '28', '27']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['89', '51', '29', '29', '28']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['90', '52', '30', '30', '29']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['91', '53', '31', '31', '30']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['92', '54', '32', '32', '31']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['93', '55', '33', '33', '32']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['94', '56', '34', '34', '33']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['95', '57', '35', '35', '34']
model: llama-7b, num requests: 6, total length: 7, prompt/kv_cache length: ['96', '58', '36', '36', '35', '2']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['97', '59', '37', '37', '36', '3']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['98', '60', '38', '38', '37', '4']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['99', '61', '39', '39', '38', '5']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['100', '62', '40', '40', '39', '6']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['101', '63', '41', '41', '40', '7']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['102', '64', '42', '42', '41', '8']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['103', '65', '43', '43', '42', '9']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['104', '66', '44', '44', '43', '10']
model: llama-7b, num requests: 6, total length: 23, prompt/kv_cache length: ['105', '67', '45', '45', '44', '18']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['106', '68', '46', '46', '45', '19']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['107', '69', '47', '47', '46', '20']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['108', '70', '48', '48', '47', '21']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['109', '71', '49', '49', '48', '22']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['110', '72', '50', '50', '49', '23']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['111', '73', '51', '51', '50', '24']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['112', '74', '52', '52', '51', '25']
[1.0s] Avg Throughput: propmt: 134, generation: 492
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['113', '75', '53', '53', '52', '26']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['114', '76', '54', '54', '53', '27']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['115', '77', '55', '55', '54', '28']
model: llama-7b, num requests: 8, total length: 94, prompt/kv_cache length: ['116', '78', '56', '56', '55', '29', '5', '83']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['117', '79', '57', '57', '56', '30', '6', '84']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['118', '80', '58', '58', '57', '31', '7', '85']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['119', '81', '59', '59', '58', '32', '8', '86']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['120', '82', '60', '60', '59', '33', '9', '87']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['121', '83', '61', '61', '60', '34', '10', '88']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['122', '84', '62', '62', '61', '35', '11', '89']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['123', '63', '63', '62', '36', '12', '90']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['124', '64', '64', '63', '37', '13', '91']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['125', '65', '65', '64', '38', '14', '92']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['66', '66', '65', '39', '15', '93']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['67', '67', '66', '40', '16', '94']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['68', '68', '67', '41', '17', '95']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['69', '69', '68', '42', '18', '96']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['70', '70', '69', '43', '19', '97']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['71', '71', '70', '44', '20', '98']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['72', '72', '71', '45', '21', '99']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['73', '73', '72', '46', '22', '100']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['74', '74', '73', '47', '23', '101']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['75', '75', '74', '48', '24', '102']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['76', '76', '75', '49', '25', '103']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['77', '77', '76', '50', '26', '104']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['78', '77', '51', '27', '105']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['79', '78', '52', '28', '106']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['80', '79', '53', '29', '107']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['81', '80', '54', '30', '108']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['82', '81', '55', '109']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['83', '82', '56', '110']
model: llama-7b, num requests: 5, total length: 12, prompt/kv_cache length: ['84', '83', '57', '111', '8']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['85', '84', '58', '112', '9']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['86', '85', '59', '113', '10']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['87', '86', '60', '114', '11']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['88', '87', '61', '115', '12']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['89', '88', '62', '116', '13']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['90', '89', '63', '117', '14']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['91', '90', '64', '118', '15']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['92', '91', '65', '119', '16']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['93', '92', '66', '120', '17']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['94', '93', '67', '121', '18']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['95', '94', '68', '122', '19']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['96', '95', '69', '123', '20']
[1.5s] Avg Throughput: propmt: 192, generation: 516
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['97', '96', '70', '124', '21']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['98', '97', '71', '125', '22']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['99', '98', '72', '126', '23']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['100', '73', '127', '24']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['101', '74', '128', '25']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['102', '75', '129', '26']
model: llama-7b, num requests: 3, total length: 3, prompt/kv_cache length: ['103', '130', '27']
model: llama-7b, num requests: 4, total length: 17, prompt/kv_cache length: ['104', '131', '28', '14']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['105', '132', '29', '15']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['106', '133', '30', '16']
model: llama-7b, num requests: 5, total length: 18, prompt/kv_cache length: ['107', '134', '31', '17', '14']
model: llama-7b, num requests: 6, total length: 8, prompt/kv_cache length: ['108', '135', '32', '18', '15', '3']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['136', '33', '19', '16', '4']
model: llama-7b, num requests: 6, total length: 13, prompt/kv_cache length: ['137', '34', '20', '17', '5', '8']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['138', '35', '21', '18', '6', '9']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['139', '36', '22', '19', '7', '10']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['140', '37', '23', '20', '8', '11']
model: llama-7b, num requests: 7, total length: 29, prompt/kv_cache length: ['141', '38', '24', '21', '9', '12', '23']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['142', '39', '25', '22', '10', '13', '24']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['143', '40', '26', '23', '11', '14', '25']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['144', '41', '27', '24', '12', '15', '26']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['145', '42', '28', '25', '13', '16', '27']
model: llama-7b, num requests: 8, total length: 25, prompt/kv_cache length: ['146', '43', '29', '26', '14', '17', '28', '18']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['44', '30', '27', '15', '18', '29', '19']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['45', '31', '28', '16', '19', '30', '20']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['46', '32', '29', '17', '20', '31', '21']
model: llama-7b, num requests: 8, total length: 37, prompt/kv_cache length: ['47', '33', '30', '18', '21', '32', '22', '30']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['48', '34', '31', '19', '22', '33', '23', '31']
model: llama-7b, num requests: 9, total length: 33, prompt/kv_cache length: ['49', '35', '32', '20', '23', '34', '24', '32', '25']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['50', '36', '33', '21', '24', '35', '25', '33', '26']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['51', '37', '34', '22', '25', '36', '26', '34', '27']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['52', '38', '35', '23', '26', '37', '27', '35', '28']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['53', '39', '36', '24', '27', '38', '28', '36', '29']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['54', '40', '37', '25', '28', '39', '29', '37', '30']
model: llama-7b, num requests: 10, total length: 17, prompt/kv_cache length: ['55', '41', '38', '26', '29', '40', '30', '38', '31', '8']
model: llama-7b, num requests: 11, total length: 20, prompt/kv_cache length: ['56', '42', '39', '27', '30', '41', '31', '39', '32', '9', '10']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['57', '43', '40', '28', '31', '42', '32', '40', '33', '10', '11']
model: llama-7b, num requests: 12, total length: 15, prompt/kv_cache length: ['58', '44', '41', '29', '32', '43', '33', '41', '34', '11', '12', '4']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['59', '45', '42', '30', '33', '44', '34', '42', '35', '12', '13', '5']
[2.0s] Avg Throughput: propmt: 314, generation: 508
model: llama-7b, num requests: 13, total length: 80, prompt/kv_cache length: ['60', '46', '43', '31', '34', '45', '35', '43', '36', '13', '14', '6', '68']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['61', '47', '44', '32', '35', '46', '36', '44', '37', '14', '15', '7', '69']
model: llama-7b, num requests: 14, total length: 20, prompt/kv_cache length: ['62', '48', '45', '33', '36', '47', '37', '45', '38', '15', '16', '8', '70', '7']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['63', '49', '46', '34', '37', '48', '38', '46', '39', '16', '17', '9', '71', '8']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['64', '50', '47', '35', '38', '49', '39', '47', '40', '17', '18', '10', '72', '9']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['65', '51', '48', '36', '39', '50', '40', '48', '41', '18', '19', '11', '73', '10']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['66', '52', '49', '37', '40', '51', '41', '49', '42', '19', '12', '74', '11']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['67', '53', '50', '38', '41', '52', '42', '50', '43', '20', '13', '75', '12']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['68', '54', '51', '39', '42', '53', '43', '44', '21', '14', '76', '13']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['69', '55', '52', '40', '43', '54', '44', '45', '22', '15', '77', '14']
model: llama-7b, num requests: 13, total length: 22, prompt/kv_cache length: ['70', '56', '53', '41', '44', '55', '45', '46', '23', '16', '78', '15', '10']
model: llama-7b, num requests: 14, total length: 29, prompt/kv_cache length: ['71', '57', '54', '42', '45', '56', '46', '47', '24', '17', '79', '16', '11', '16']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['72', '58', '55', '43', '46', '57', '47', '48', '25', '18', '80', '17', '12', '17']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['73', '59', '56', '44', '47', '58', '48', '49', '26', '19', '81', '18', '13', '18']
model: llama-7b, num requests: 15, total length: 22, prompt/kv_cache length: ['74', '60', '57', '45', '48', '59', '49', '50', '27', '20', '82', '19', '14', '19', '8']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['75', '61', '58', '46', '49', '60', '50', '51', '28', '21', '83', '20', '15', '20', '9']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['76', '62', '59', '47', '50', '61', '51', '52', '29', '22', '84', '21', '16', '21', '10']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['77', '63', '60', '48', '51', '62', '52', '53', '30', '23', '85', '22', '17', '22', '11']
model: llama-7b, num requests: 17, total length: 75, prompt/kv_cache length: ['78', '64', '61', '49', '52', '63', '53', '54', '31', '24', '86', '23', '18', '23', '12', '47', '13']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['79', '65', '62', '50', '53', '64', '54', '55', '32', '25', '87', '24', '19', '24', '13', '48', '14']
[2.5s] Avg Throughput: propmt: 338, generation: 538
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['80', '66', '63', '51', '54', '65', '55', '56', '33', '26', '88', '25', '25', '14', '49', '15']
model: llama-7b, num requests: 17, total length: 42, prompt/kv_cache length: ['81', '67', '64', '52', '55', '66', '56', '57', '34', '27', '89', '26', '26', '15', '50', '16', '26']
model: llama-7b, num requests: 19, total length: 68, prompt/kv_cache length: ['82', '68', '65', '53', '56', '67', '57', '58', '35', '28', '90', '27', '27', '16', '51', '17', '27', '8', '43']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['83', '69', '66', '54', '57', '68', '58', '59', '36', '29', '91', '28', '28', '17', '52', '18', '28', '9', '44']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['84', '70', '67', '55', '58', '69', '59', '60', '30', '92', '29', '29', '18', '53', '19', '29', '10', '45']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['85', '71', '68', '56', '59', '70', '60', '61', '31', '93', '30', '30', '19', '54', '20', '30', '11', '46']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['86', '72', '69', '57', '60', '71', '61', '62', '32', '94', '31', '31', '20', '55', '21', '31', '12', '47']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['87', '73', '70', '58', '61', '72', '62', '63', '33', '95', '32', '32', '21', '56', '22', '32', '13', '48']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['88', '74', '71', '59', '62', '73', '63', '64', '34', '96', '33', '33', '22', '57', '23', '33', '14', '49']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['89', '75', '72', '60', '63', '74', '64', '65', '35', '97', '34', '34', '23', '58', '24', '34', '15', '50']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['90', '76', '73', '61', '64', '75', '65', '66', '36', '98', '35', '35', '24', '59', '25', '35', '16', '51']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['91', '77', '74', '62', '65', '76', '66', '67', '37', '99', '36', '36', '25', '60', '26', '36', '17', '52']
model: llama-7b, num requests: 19, total length: 61, prompt/kv_cache length: ['92', '78', '75', '63', '66', '77', '67', '68', '38', '100', '37', '37', '26', '61', '27', '37', '18', '53', '43']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['93', '79', '76', '64', '67', '78', '68', '69', '39', '101', '38', '38', '27', '62', '28', '38', '19', '54', '44']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['94', '80', '77', '65', '68', '79', '69', '70', '40', '102', '39', '39', '28', '63', '29', '39', '20', '55', '45']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['95', '81', '78', '66', '69', '80', '70', '71', '41', '103', '40', '40', '29', '64', '30', '40', '21', '56', '46']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['96', '82', '79', '67', '70', '81', '71', '72', '42', '104', '41', '41', '30', '65', '31', '41', '22', '57', '47']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['97', '83', '80', '68', '71', '82', '72', '73', '43', '105', '42', '42', '31', '66', '32', '42', '23', '58', '48']
[3.0s] Avg Throughput: propmt: 240, generation: 646
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['98', '84', '81', '69', '72', '73', '74', '44', '106', '43', '43', '32', '67', '33', '43', '24', '59', '49']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['85', '82', '70', '73', '74', '75', '45', '107', '44', '44', '33', '68', '34', '44', '25', '60', '50']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['86', '83', '71', '74', '75', '76', '46', '108', '45', '45', '34', '69', '35', '45', '26', '61', '51']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['87', '84', '72', '75', '76', '77', '47', '109', '46', '46', '35', '70', '36', '46', '27', '62', '52']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['88', '85', '73', '76', '77', '78', '48', '110', '47', '47', '36', '71', '37', '47', '28', '63', '53']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['89', '86', '74', '77', '78', '49', '111', '48', '48', '37', '72', '38', '48', '29', '64', '54']
model: llama-7b, num requests: 17, total length: 33, prompt/kv_cache length: ['90', '87', '75', '78', '79', '50', '112', '49', '49', '38', '73', '39', '49', '30', '65', '55', '17']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['91', '88', '76', '79', '80', '51', '113', '50', '50', '39', '74', '40', '50', '31', '66', '18']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['92', '89', '77', '80', '81', '52', '114', '51', '51', '40', '75', '41', '51', '32', '67', '19']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['93', '90', '78', '81', '82', '53', '115', '52', '52', '41', '76', '42', '52', '33', '68', '20']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['94', '91', '79', '82', '83', '116', '53', '53', '42', '77', '43', '53', '34', '69', '21']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['95', '92', '80', '83', '84', '117', '54', '54', '43', '44', '54', '70', '22']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['96', '93', '81', '84', '85', '118', '55', '55', '44', '45', '55', '71', '23']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['97', '94', '82', '85', '86', '119', '56', '56', '45', '46', '56', '72', '24']
model: llama-7b, num requests: 14, total length: 21, prompt/kv_cache length: ['98', '95', '83', '86', '87', '120', '57', '57', '46', '47', '57', '73', '25', '8']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['99', '96', '84', '87', '88', '121', '58', '58', '47', '48', '58', '74', '26', '9', '1']
model: llama-7b, num requests: 15, total length: 23, prompt/kv_cache length: ['97', '85', '88', '89', '122', '59', '59', '48', '49', '59', '75', '27', '10', '2', '9']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['98', '86', '89', '90', '123', '60', '60', '49', '50', '60', '76', '28', '11', '3', '10']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['99', '87', '90', '91', '124', '61', '61', '50', '51', '61', '77', '29', '12', '4', '11']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['100', '88', '91', '92', '125', '62', '62', '51', '52', '62', '78', '30', '13', '5', '12']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['101', '89', '92', '93', '126', '63', '63', '52', '53', '63', '79', '31', '14', '6', '13']
[3.5s] Avg Throughput: propmt: 70, generation: 650
model: llama-7b, num requests: 16, total length: 17, prompt/kv_cache length: ['102', '90', '93', '94', '127', '64', '64', '53', '54', '64', '80', '32', '15', '7', '14', '2']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['103', '91', '94', '95', '128', '65', '65', '54', '55', '65', '81', '33', '16', '8', '15', '3']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['104', '92', '95', '96', '129', '66', '66', '55', '56', '66', '82', '34', '17', '9', '16', '4', '1']
model: llama-7b, num requests: 18, total length: 120, prompt/kv_cache length: ['105', '93', '96', '97', '130', '67', '67', '56', '57', '67', '83', '35', '18', '10', '17', '5', '2', '103']
model: llama-7b, num requests: 20, total length: 86, prompt/kv_cache length: ['106', '94', '97', '98', '131', '68', '68', '57', '58', '68', '84', '36', '19', '11', '18', '6', '3', '104', '63', '5']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['107', '95', '98', '99', '132', '69', '69', '58', '59', '69', '85', '37', '20', '12', '19', '7', '4', '64', '6']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['108', '96', '99', '100', '133', '70', '70', '59', '60', '70', '86', '38', '21', '13', '20', '8', '5', '65', '7']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['109', '97', '100', '101', '134', '71', '71', '60', '61', '71', '87', '39', '22', '14', '21', '9', '6', '66', '8']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['110', '98', '101', '102', '135', '72', '72', '61', '62', '72', '88', '40', '23', '15', '22', '10', '7', '67', '9']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['111', '99', '102', '103', '136', '73', '73', '62', '63', '73', '89', '41', '24', '16', '23', '11', '8', '68', '10']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['112', '100', '103', '104', '137', '74', '74', '63', '64', '74', '90', '42', '25', '17', '24', '12', '9', '69', '11']
model: llama-7b, num requests: 19, total length: 35, prompt/kv_cache length: ['101', '104', '105', '138', '75', '75', '64', '65', '75', '91', '43', '26', '18', '25', '13', '10', '70', '12', '17']
model: llama-7b, num requests: 20, total length: 25, prompt/kv_cache length: ['102', '105', '106', '139', '76', '76', '65', '66', '76', '92', '44', '27', '19', '26', '14', '11', '71', '13', '18', '6']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['103', '106', '107', '140', '77', '77', '66', '67', '77', '93', '45', '28', '20', '27', '15', '12', '72', '14', '19', '7']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['104', '107', '108', '141', '78', '78', '67', '68', '78', '94', '46', '29', '21', '28', '16', '13', '73', '15', '20', '8']
[4.0s] Avg Throughput: propmt: 394, generation: 536
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['105', '108', '109', '142', '79', '79', '68', '69', '79', '95', '47', '30', '22', '29', '17', '14', '74', '16', '21', '9']
model: llama-7b, num requests: 21, total length: 40, prompt/kv_cache length: ['106', '109', '110', '143', '80', '80', '69', '70', '80', '96', '48', '31', '23', '30', '18', '15', '75', '17', '22', '10', '20']
model: llama-7b, num requests: 21, total length: 56, prompt/kv_cache length: ['110', '111', '144', '81', '81', '70', '71', '81', '97', '49', '32', '24', '31', '19', '16', '76', '18', '23', '11', '21', '36']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['111', '112', '145', '82', '82', '71', '72', '82', '98', '50', '33', '25', '32', '20', '17', '77', '19', '24', '12', '22', '37']
model: llama-7b, num requests: 22, total length: 130, prompt/kv_cache length: ['112', '113', '146', '83', '83', '72', '73', '83', '99', '51', '34', '26', '33', '21', '18', '78', '20', '25', '13', '23', '38', '109']
model: llama-7b, num requests: 24, total length: 104, prompt/kv_cache length: ['113', '114', '147', '84', '84', '73', '74', '84', '100', '52', '35', '27', '34', '22', '19', '79', '21', '26', '14', '24', '39', '110', '9', '73']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['114', '115', '148', '85', '85', '74', '75', '85', '101', '53', '36', '35', '20', '80', '22', '27', '15', '25', '40', '111', '10', '74']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['115', '116', '149', '86', '86', '75', '76', '86', '102', '54', '37', '36', '81', '23', '28', '16', '26', '41', '112', '11', '75']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['116', '117', '150', '87', '87', '76', '77', '87', '103', '55', '38', '37', '82', '24', '29', '17', '27', '42', '113', '12', '76']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['117', '118', '151', '88', '88', '77', '78', '88', '104', '56', '39', '38', '83', '25', '30', '18', '28', '43', '114', '13', '77']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['118', '119', '152', '89', '89', '78', '79', '89', '105', '57', '40', '39', '84', '26', '31', '19', '29', '44', '115', '14', '78']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['119', '120', '153', '90', '90', '79', '80', '90', '106', '58', '41', '40', '85', '27', '32', '20', '30', '45', '116', '15', '79']
[4.5s] Avg Throughput: propmt: 494, generation: 500
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['120', '121', '154', '91', '91', '80', '81', '91', '107', '59', '42', '41', '86', '28', '33', '21', '31', '46', '117', '16', '80']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['121', '122', '155', '92', '92', '81', '82', '92', '108', '60', '43', '42', '87', '29', '34', '22', '32', '47', '118', '17', '81']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['122', '123', '156', '93', '93', '82', '83', '93', '109', '61', '44', '43', '88', '30', '35', '23', '33', '48', '119', '18', '82']
model: llama-7b, num requests: 22, total length: 33, prompt/kv_cache length: ['123', '124', '157', '94', '94', '83', '84', '94', '110', '62', '45', '44', '89', '31', '36', '24', '34', '49', '120', '19', '83', '12']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['124', '125', '158', '95', '84', '85', '95', '111', '63', '46', '45', '90', '32', '37', '25', '35', '50', '121', '20', '84', '13']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['125', '126', '159', '96', '85', '86', '96', '112', '64', '47', '46', '91', '33', '38', '26', '36', '51', '122', '21', '85', '14']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['126', '127', '160', '97', '86', '87', '97', '113', '65', '48', '47', '92', '34', '39', '27', '37', '52', '123', '22', '86', '15']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['128', '161', '98', '87', '88', '98', '114', '66', '49', '48', '93', '35', '40', '28', '38', '53', '124', '23', '87', '16']
model: llama-7b, num requests: 21, total length: 24, prompt/kv_cache length: ['129', '162', '99', '88', '89', '99', '115', '67', '50', '49', '94', '36', '41', '29', '39', '54', '125', '24', '88', '17', '4']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['130', '163', '100', '89', '90', '100', '116', '68', '51', '50', '95', '37', '42', '30', '40', '55', '126', '25', '89', '18', '5']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['131', '164', '101', '90', '91', '101', '117', '69', '52', '51', '96', '38', '43', '31', '41', '56', '127', '26', '90', '19', '6']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['132', '165', '102', '92', '102', '118', '70', '53', '52', '97', '39', '44', '32', '42', '57', '128', '27', '91', '20', '7']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['133', '166', '103', '93', '103', '119', '71', '54', '53', '98', '40', '45', '33', '43', '58', '129', '28', '92', '21', '8']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['134', '167', '104', '94', '104', '120', '72', '54', '99', '41', '46', '34', '44', '59', '130', '29', '93', '22', '9']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['135', '168', '105', '95', '105', '121', '73', '55', '100', '42', '35', '45', '60', '131', '30', '94', '23', '10']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['136', '169', '106', '96', '106', '122', '74', '56', '101', '43', '36', '61', '132', '31', '95', '24', '11']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['137', '170', '107', '97', '107', '123', '75', '57', '102', '44', '37', '62', '133', '32', '96', '25', '12']
[5.0s] Avg Throughput: propmt: 32, generation: 688
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['138', '171', '108', '98', '108', '124', '76', '58', '103', '45', '38', '63', '134', '33', '97', '26']
model: llama-7b, num requests: 17, total length: 32, prompt/kv_cache length: ['139', '172', '109', '99', '109', '125', '77', '59', '104', '46', '39', '64', '135', '34', '98', '27', '16']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['173', '110', '100', '110', '126', '78', '105', '47', '40', '65', '136', '35', '99', '28', '17']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['174', '111', '101', '111', '127', '79', '106', '48', '41', '66', '137', '36', '100', '29', '18']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['175', '112', '102', '112', '128', '80', '107', '49', '42', '67', '138', '37', '101', '30', '19']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['176', '113', '103', '113', '129', '81', '108', '50', '43', '68', '139', '38', '102', '31', '20']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['177', '114', '104', '114', '130', '82', '109', '51', '44', '69', '140', '39', '103', '32', '21']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['178', '115', '105', '115', '131', '83', '110', '52', '45', '70', '141', '40', '104', '33', '22']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['179', '116', '106', '116', '132', '111', '53', '46', '71', '142', '41', '105', '34', '23']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['180', '107', '117', '133', '112', '54', '47', '72', '143', '42', '106', '35', '24']
model: llama-7b, num requests: 13, total length: 61, prompt/kv_cache length: ['108', '118', '134', '113', '55', '48', '73', '144', '43', '107', '36', '25', '49']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['109', '119', '135', '114', '56', '49', '74', '145', '44', '108', '37', '26', '50']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['110', '120', '136', '115', '57', '50', '75', '146', '45', '109', '38', '27', '51']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['111', '121', '137', '116', '58', '51', '76', '147', '46', '110', '39', '28', '52']
model: llama-7b, num requests: 14, total length: 22, prompt/kv_cache length: ['112', '122', '138', '117', '59', '52', '77', '148', '47', '111', '40', '29', '53', '9']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['113', '123', '139', '118', '60', '53', '78', '149', '48', '41', '30', '54', '10']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['114', '124', '140', '119', '61', '54', '79', '150', '49', '42', '31', '55', '11']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['115', '125', '141', '120', '62', '55', '80', '151', '50', '43', '32', '56', '12']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['116', '126', '142', '121', '63', '56', '81', '152', '51', '44', '33', '57', '13']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['117', '127', '143', '122', '64', '57', '82', '153', '52', '45', '34', '58', '14']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['118', '128', '144', '123', '65', '58', '83', '154', '53', '46', '59', '15']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['119', '145', '124', '66', '59', '84', '155', '54', '47', '60', '16']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['120', '146', '125', '67', '60', '85', '156', '55', '48', '61', '17']
[5.5s] Avg Throughput: propmt: 148, generation: 636
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['121', '147', '126', '68', '61', '86', '157', '56', '49', '62', '18']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['122', '148', '127', '69', '62', '87', '158', '57', '50', '63', '19']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['123', '149', '128', '70', '63', '88', '159', '58', '51', '64', '20']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['124', '150', '129', '71', '64', '89', '160', '59', '52', '65', '21']
model: llama-7b, num requests: 12, total length: 50, prompt/kv_cache length: ['125', '151', '130', '72', '65', '90', '161', '60', '53', '66', '22', '39']
model: llama-7b, num requests: 14, total length: 127, prompt/kv_cache length: ['126', '152', '131', '73', '66', '91', '162', '61', '54', '67', '23', '40', '91', '24']
model: llama-7b, num requests: 16, total length: 158, prompt/kv_cache length: ['127', '153', '132', '74', '67', '92', '163', '62', '55', '68', '24', '41', '92', '25', '24', '120']
model: llama-7b, num requests: 18, total length: 53, prompt/kv_cache length: ['128', '154', '133', '75', '68', '93', '164', '63', '56', '69', '25', '42', '93', '26', '25', '121', '18', '19']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['129', '134', '76', '69', '94', '165', '64', '57', '70', '26', '43', '94', '27', '26', '122', '19', '20']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['130', '135', '77', '70', '95', '166', '65', '58', '71', '27', '44', '28', '123', '20', '21']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['136', '78', '96', '167', '66', '59', '72', '28', '45', '29', '124', '21', '22']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['137', '79', '97', '168', '67', '60', '73', '29', '46', '30', '125', '22', '23']
model: llama-7b, num requests: 14, total length: 27, prompt/kv_cache length: ['138', '80', '98', '169', '68', '61', '74', '30', '47', '31', '126', '23', '24', '14']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['139', '81', '99', '170', '69', '62', '75', '31', '48', '32', '127', '24', '25', '15']
model: llama-7b, num requests: 15, total length: 20, prompt/kv_cache length: ['140', '82', '100', '171', '70', '63', '76', '32', '49', '33', '128', '25', '26', '16', '6']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['141', '83', '101', '172', '71', '64', '77', '33', '50', '34', '129', '26', '27', '17', '7']
[6.0s] Avg Throughput: propmt: 710, generation: 414
model: llama-7b, num requests: 16, total length: 42, prompt/kv_cache length: ['142', '84', '102', '173', '72', '65', '78', '34', '51', '35', '130', '27', '28', '18', '8', '27']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['143', '85', '103', '174', '73', '66', '79', '35', '52', '36', '131', '28', '29', '19', '9', '28']
model: llama-7b, num requests: 17, total length: 28, prompt/kv_cache length: ['144', '86', '104', '175', '74', '67', '80', '36', '53', '37', '132', '29', '30', '20', '10', '29', '12']
model: llama-7b, num requests: 18, total length: 36, prompt/kv_cache length: ['145', '87', '105', '176', '75', '68', '81', '37', '54', '38', '133', '30', '31', '21', '11', '30', '13', '19']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['146', '88', '106', '177', '76', '69', '82', '38', '55', '39', '134', '31', '32', '22', '12', '31', '14', '20']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['147', '89', '107', '178', '77', '70', '83', '39', '56', '40', '135', '32', '33', '23', '13', '32', '15', '21']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['148', '90', '108', '179', '78', '71', '84', '40', '57', '41', '136', '33', '34', '24', '14', '33', '16', '22']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['149', '91', '109', '180', '79', '72', '85', '41', '58', '42', '137', '34', '35', '25', '15', '34', '17', '23']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['92', '110', '181', '80', '73', '86', '42', '59', '43', '138', '35', '36', '26', '16', '35', '18', '24']
model: llama-7b, num requests: 18, total length: 56, prompt/kv_cache length: ['93', '111', '182', '81', '74', '43', '60', '44', '139', '36', '37', '27', '17', '36', '19', '25', '36', '4']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['94', '112', '183', '82', '75', '44', '61', '45', '140', '37', '38', '28', '18', '37', '20', '26', '37', '5']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['95', '113', '184', '83', '76', '45', '62', '46', '141', '38', '39', '29', '19', '38', '21', '27', '38', '6']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['96', '114', '185', '84', '77', '46', '63', '47', '142', '39', '40', '30', '20', '39', '22', '28', '39', '7']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['97', '115', '186', '85', '78', '47', '64', '48', '143', '40', '41', '31', '21', '40', '23', '29', '40', '8']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['98', '116', '187', '86', '79', '48', '65', '49', '144', '41', '42', '32', '22', '41', '24', '30', '41', '9']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['99', '117', '188', '87', '80', '49', '66', '50', '145', '42', '43', '33', '23', '42', '25', '31', '42', '10']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['100', '118', '189', '88', '81', '50', '67', '51', '146', '43', '44', '34', '24', '43', '26', '32', '43', '11']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['101', '119', '190', '89', '82', '51', '68', '52', '147', '44', '45', '35', '25', '44', '27', '33', '44', '12']
[6.5s] Avg Throughput: propmt: 196, generation: 620
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['102', '120', '191', '83', '52', '69', '53', '148', '45', '46', '36', '26', '45', '28', '34', '45', '13']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['103', '121', '192', '84', '53', '54', '149', '46', '47', '37', '27', '46', '29', '35', '46', '14']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['104', '122', '193', '54', '55', '150', '47', '48', '38', '28', '47', '30', '36', '47', '15']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['105', '123', '194', '55', '56', '151', '48', '49', '39', '48', '31', '37', '48', '16']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['106', '124', '195', '56', '152', '49', '50', '40', '49', '32', '38', '49', '17']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['107', '125', '196', '57', '153', '50', '51', '41', '50', '33', '39', '50', '18']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['108', '126', '197', '58', '154', '51', '52', '42', '51', '34', '40', '51', '19']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['109', '127', '198', '59', '155', '52', '53', '43', '52', '35', '41', '52', '20']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['110', '128', '199', '60', '156', '53', '54', '44', '53', '36', '42', '53', '21']
model: llama-7b, num requests: 14, total length: 20, prompt/kv_cache length: ['111', '129', '200', '61', '157', '54', '55', '45', '54', '37', '43', '54', '22', '7']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['112', '130', '201', '62', '158', '55', '56', '46', '55', '38', '44', '55', '23', '8']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['113', '131', '202', '63', '159', '56', '57', '47', '56', '39', '45', '56', '24', '9']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['114', '132', '203', '64', '160', '57', '58', '48', '57', '40', '46', '57', '25', '10']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['115', '133', '204', '65', '161', '58', '59', '49', '58', '41', '47', '58', '26', '11']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['116', '134', '205', '66', '162', '59', '60', '50', '59', '42', '48', '59', '27', '12']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['117', '135', '206', '67', '163', '60', '61', '51', '60', '43', '49', '60', '28', '13']
model: llama-7b, num requests: 15, total length: 63, prompt/kv_cache length: ['118', '136', '207', '68', '164', '61', '62', '52', '61', '44', '50', '61', '29', '14', '49']
model: llama-7b, num requests: 17, total length: 59, prompt/kv_cache length: ['119', '137', '208', '69', '165', '62', '63', '53', '62', '45', '51', '62', '30', '15', '50', '13', '31']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['120', '138', '209', '70', '166', '63', '64', '54', '63', '46', '52', '63', '31', '16', '51', '14', '32']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['121', '139', '210', '71', '167', '64', '65', '55', '64', '47', '53', '64', '32', '17', '52', '15', '33']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['122', '140', '211', '72', '168', '65', '66', '56', '65', '48', '54', '65', '33', '18', '53', '16', '34']
[7.0s] Avg Throughput: propmt: 200, generation: 610
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['123', '141', '212', '73', '169', '66', '67', '57', '66', '49', '55', '66', '34', '19', '54', '17', '35']
model: llama-7b, num requests: 16, total length: 26, prompt/kv_cache length: ['142', '74', '170', '67', '68', '58', '67', '50', '56', '67', '35', '20', '55', '18', '36', '11']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['143', '75', '171', '68', '69', '59', '68', '51', '57', '68', '36', '56', '19', '37']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['144', '76', '172', '69', '70', '60', '69', '52', '58', '69', '37', '57', '20', '38']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['145', '77', '173', '70', '71', '61', '70', '53', '59', '70', '38', '58', '21', '39']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['146', '78', '174', '71', '72', '62', '71', '54', '60', '71', '39', '59', '22', '40']
model: llama-7b, num requests: 14, total length: 14, prompt/kv_cache length: ['147', '79', '175', '72', '73', '63', '72', '55', '61', '72', '40', '60', '23', '41']
model: llama-7b, num requests: 15, total length: 36, prompt/kv_cache length: ['148', '80', '176', '73', '74', '64', '73', '56', '62', '73', '41', '61', '24', '42', '22']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['149', '81', '177', '74', '75', '65', '74', '57', '63', '74', '42', '62', '25', '43', '23']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['150', '82', '178', '75', '76', '66', '75', '58', '64', '75', '43', '63', '26', '44', '24']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['151', '83', '179', '76', '77', '67', '76', '59', '65', '76', '44', '64', '27', '45', '25']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['152', '84', '180', '77', '78', '68', '77', '60', '66', '77', '45', '65', '28', '46', '26']
model: llama-7b, num requests: 16, total length: 29, prompt/kv_cache length: ['153', '85', '181', '78', '79', '69', '78', '61', '67', '78', '46', '66', '29', '47', '27', '14']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['154', '86', '182', '79', '80', '70', '79', '62', '68', '79', '47', '67', '30', '48', '28', '15']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['155', '87', '183', '80', '81', '71', '80', '63', '69', '80', '48', '68', '31', '49', '29', '16']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['156', '88', '184', '81', '82', '72', '81', '64', '70', '81', '49', '69', '32', '50', '30', '17']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['157', '89', '185', '82', '83', '73', '82', '65', '71', '82', '50', '70', '33', '51', '31', '18']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['158', '90', '186', '83', '84', '74', '83', '66', '72', '83', '51', '71', '34', '52', '32', '19']
model: llama-7b, num requests: 17, total length: 30, prompt/kv_cache length: ['159', '91', '187', '84', '85', '75', '84', '67', '73', '84', '52', '72', '35', '53', '33', '20', '14']
model: llama-7b, num requests: 18, total length: 32, prompt/kv_cache length: ['160', '92', '188', '85', '86', '76', '85', '68', '74', '85', '53', '73', '36', '54', '34', '21', '15', '15']
model: llama-7b, num requests: 19, total length: 54, prompt/kv_cache length: ['161', '189', '86', '87', '77', '86', '69', '75', '86', '54', '74', '37', '55', '35', '22', '16', '16', '32', '5']
[7.5s] Avg Throughput: propmt: 152, generation: 642
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['190', '87', '88', '78', '87', '70', '76', '87', '55', '75', '38', '56', '36', '23', '17', '17', '33', '6']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['191', '88', '89', '79', '88', '71', '77', '88', '56', '76', '39', '57', '37', '24', '18', '18', '34', '7']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['192', '89', '90', '80', '89', '72', '78', '89', '57', '77', '40', '58', '38', '25', '19', '19', '35', '8']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['193', '90', '91', '81', '90', '73', '79', '90', '58', '78', '41', '59', '39', '26', '20', '20', '36', '9']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['194', '91', '82', '91', '74', '91', '59', '79', '42', '60', '40', '27', '21', '21', '37', '10']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['195', '92', '83', '92', '75', '92', '60', '80', '43', '61', '41', '28', '22', '22', '38', '11']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['196', '93', '84', '93', '76', '93', '61', '81', '44', '62', '42', '29', '23', '23', '39', '12']
model: llama-7b, num requests: 17, total length: 59, prompt/kv_cache length: ['197', '94', '85', '94', '77', '94', '62', '82', '45', '63', '43', '30', '24', '24', '40', '13', '43']
model: llama-7b, num requests: 17, total length: 17, prompt/kv_cache length: ['198', '95', '86', '95', '78', '95', '63', '83', '46', '64', '44', '31', '25', '25', '41', '14', '44']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['199', '96', '96', '79', '96', '64', '84', '47', '65', '45', '32', '26', '26', '42', '15', '45']
model: llama-7b, num requests: 17, total length: 32, prompt/kv_cache length: ['200', '97', '97', '80', '97', '65', '85', '48', '66', '46', '33', '27', '27', '43', '16', '46', '16']
model: llama-7b, num requests: 17, total length: 22, prompt/kv_cache length: ['201', '98', '98', '81', '98', '86', '49', '67', '47', '34', '28', '28', '44', '17', '47', '17', '6']
model: llama-7b, num requests: 18, total length: 38, prompt/kv_cache length: ['202', '99', '99', '82', '99', '87', '50', '68', '48', '35', '29', '29', '45', '18', '48', '18', '7', '21']
model: llama-7b, num requests: 19, total length: 23, prompt/kv_cache length: ['203', '100', '100', '83', '100', '88', '51', '69', '49', '36', '30', '30', '46', '19', '49', '19', '8', '22', '5']
model: llama-7b, num requests: 20, total length: 29, prompt/kv_cache length: ['204', '101', '101', '84', '101', '89', '52', '70', '50', '37', '31', '31', '47', '20', '50', '20', '9', '23', '6', '10']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['205', '102', '102', '85', '102', '90', '53', '71', '51', '38', '32', '32', '48', '21', '51', '21', '10', '24', '7', '11']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['206', '103', '103', '86', '103', '91', '54', '72', '52', '39', '33', '33', '49', '22', '52', '22', '11', '25', '8', '12']
[8.0s] Avg Throughput: propmt: 276, generation: 584
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['207', '104', '104', '87', '104', '92', '55', '73', '53', '40', '34', '34', '50', '23', '53', '23', '12', '26', '9', '13']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['208', '105', '105', '88', '105', '93', '56', '74', '54', '41', '35', '35', '51', '24', '54', '24', '13', '27', '10', '14']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['209', '106', '106', '89', '106', '94', '57', '75', '55', '42', '36', '36', '52', '25', '55', '25', '14', '28', '11', '15', '1']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['210', '107', '107', '90', '107', '95', '58', '76', '56', '43', '37', '37', '53', '26', '56', '26', '15', '29', '12', '16', '2']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['211', '108', '108', '91', '108', '96', '59', '77', '57', '44', '38', '38', '54', '27', '57', '27', '16', '30', '13', '17', '3']
model: llama-7b, num requests: 22, total length: 37, prompt/kv_cache length: ['212', '109', '109', '92', '109', '97', '60', '78', '58', '45', '39', '39', '55', '28', '58', '28', '17', '31', '14', '18', '4', '16']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['213', '110', '110', '93', '110', '98', '61', '79', '59', '46', '40', '40', '56', '29', '59', '29', '18', '32', '15', '19', '5', '17']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['214', '111', '111', '94', '111', '99', '62', '80', '60', '47', '41', '41', '57', '30', '60', '30', '19', '33', '16', '20', '6', '18']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['215', '112', '112', '95', '112', '100', '63', '81', '61', '48', '42', '42', '58', '31', '61', '31', '20', '34', '17', '21', '7', '19']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['216', '113', '113', '96', '113', '101', '64', '82', '62', '49', '43', '43', '59', '32', '62', '32', '21', '35', '18', '22', '8', '20']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['217', '114', '114', '97', '114', '102', '65', '83', '63', '50', '44', '44', '60', '33', '63', '33', '22', '36', '19', '23', '9', '21']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['218', '115', '115', '98', '115', '103', '66', '84', '64', '51', '45', '45', '61', '34', '64', '34', '23', '37', '20', '24', '10', '22']
model: llama-7b, num requests: 23, total length: 88, prompt/kv_cache length: ['219', '116', '116', '99', '116', '104', '67', '85', '65', '52', '46', '46', '62', '35', '65', '35', '24', '38', '21', '25', '11', '23', '66']
model: llama-7b, num requests: 24, total length: 44, prompt/kv_cache length: ['220', '117', '117', '100', '117', '68', '86', '66', '53', '47', '47', '63', '36', '66', '36', '25', '39', '22', '26', '12', '24', '67', '9', '13']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['221', '118', '118', '101', '118', '69', '87', '67', '54', '48', '48', '64', '37', '67', '37', '26', '40', '23', '27', '13', '25', '68', '10', '14']
[8.5s] Avg Throughput: propmt: 210, generation: 638
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['222', '119', '119', '102', '119', '70', '88', '68', '55', '49', '49', '65', '38', '68', '38', '27', '41', '24', '28', '14', '26', '69', '11', '15']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['223', '120', '120', '103', '120', '71', '89', '69', '56', '50', '50', '66', '39', '69', '39', '28', '42', '25', '29', '15', '27', '70', '12', '16']
model: llama-7b, num requests: 25, total length: 35, prompt/kv_cache length: ['224', '121', '121', '104', '121', '72', '90', '70', '57', '51', '51', '67', '40', '70', '40', '29', '43', '26', '30', '16', '28', '71', '13', '17', '11']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['225', '122', '105', '122', '73', '91', '71', '58', '52', '52', '68', '41', '71', '41', '30', '44', '27', '31', '17', '29', '72', '14', '18', '12']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['226', '123', '106', '123', '74', '92', '72', '59', '53', '53', '69', '42', '72', '42', '31', '45', '28', '32', '18', '30', '73', '15', '19', '13']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['227', '124', '107', '124', '75', '93', '73', '60', '54', '54', '70', '43', '73', '43', '32', '46', '29', '33', '19', '31', '74', '16', '20', '14']
model: llama-7b, num requests: 25, total length: 36, prompt/kv_cache length: ['228', '125', '108', '125', '76', '94', '74', '61', '55', '55', '71', '44', '74', '44', '33', '47', '30', '34', '20', '32', '75', '17', '21', '15', '12']
model: llama-7b, num requests: 25, total length: 25, prompt/kv_cache length: ['229', '126', '109', '126', '77', '95', '75', '62', '56', '56', '72', '45', '75', '45', '34', '48', '31', '35', '21', '33', '76', '18', '22', '16', '13']
model: llama-7b, num requests: 25, total length: 31, prompt/kv_cache length: ['230', '127', '110', '78', '96', '76', '63', '57', '57', '73', '46', '76', '46', '35', '49', '32', '36', '22', '34', '77', '19', '23', '17', '14', '7']
model: llama-7b, num requests: 25, total length: 25, prompt/kv_cache length: ['231', '128', '111', '79', '97', '77', '64', '58', '58', '74', '47', '77', '47', '36', '50', '33', '37', '23', '35', '78', '20', '24', '18', '15', '8']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['232', '129', '112', '80', '78', '65', '59', '59', '75', '48', '78', '48', '37', '51', '34', '38', '24', '36', '79', '21', '25', '19', '16', '9']
model: llama-7b, num requests: 23, total length: 23, prompt/kv_cache length: ['233', '130', '113', '81', '79', '66', '60', '60', '76', '49', '79', '49', '38', '52', '35', '39', '37', '80', '22', '26', '20', '17', '10']
model: llama-7b, num requests: 23, total length: 33, prompt/kv_cache length: ['234', '131', '114', '82', '80', '67', '61', '61', '77', '50', '80', '50', '39', '53', '36', '40', '38', '81', '27', '21', '18', '11', '11']
model: llama-7b, num requests: 23, total length: 23, prompt/kv_cache length: ['235', '132', '115', '83', '81', '68', '62', '62', '78', '51', '81', '51', '40', '54', '37', '41', '39', '82', '28', '22', '19', '12', '12']
[9.0s] Avg Throughput: propmt: 82, generation: 670
model: llama-7b, num requests: 24, total length: 38, prompt/kv_cache length: ['236', '133', '116', '84', '82', '69', '63', '63', '79', '52', '82', '52', '41', '55', '38', '42', '40', '83', '29', '23', '20', '13', '13', '15']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['237', '134', '117', '85', '83', '70', '64', '64', '80', '53', '83', '53', '42', '56', '39', '43', '41', '84', '30', '24', '21', '14', '14', '16']
model: llama-7b, num requests: 22, total length: 43, prompt/kv_cache length: ['118', '86', '84', '71', '65', '65', '81', '54', '84', '54', '43', '57', '40', '44', '42', '85', '31', '25', '22', '15', '17', '22']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['119', '87', '85', '72', '66', '66', '82', '55', '85', '55', '44', '58', '41', '45', '43', '86', '32', '26', '23', '16', '18', '23']
model: llama-7b, num requests: 24, total length: 86, prompt/kv_cache length: ['88', '86', '73', '67', '67', '83', '56', '86', '56', '45', '59', '42', '46', '44', '87', '33', '27', '24', '17', '19', '24', '42', '5', '18']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['89', '87', '74', '68', '68', '84', '57', '87', '57', '46', '60', '43', '47', '45', '88', '34', '28', '25', '18', '20', '25', '43', '6', '19']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['90', '88', '75', '69', '69', '85', '58', '88', '58', '47', '61', '44', '48', '46', '89', '35', '29', '26', '19', '21', '26', '44', '7', '20']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['91', '89', '76', '70', '70', '86', '59', '89', '59', '48', '62', '45', '49', '47', '90', '36', '30', '27', '20', '22', '27', '45', '8', '21']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['92', '90', '77', '71', '71', '87', '60', '90', '60', '49', '63', '46', '50', '48', '91', '37', '31', '28', '21', '23', '28', '46', '9', '22']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['93', '91', '78', '72', '72', '88', '61', '91', '61', '50', '64', '47', '51', '49', '92', '38', '32', '29', '22', '24', '29', '47', '10', '23']
model: llama-7b, num requests: 24, total length: 24, prompt/kv_cache length: ['94', '92', '79', '73', '73', '89', '62', '92', '62', '51', '65', '48', '52', '50', '93', '39', '33', '30', '23', '25', '30', '48', '11', '24']
model: llama-7b, num requests: 23, total length: 23, prompt/kv_cache length: ['95', '93', '80', '74', '74', '90', '63', '93', '63', '52', '66', '49', '53', '51', '40', '34', '31', '24', '26', '31', '49', '12', '25']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['96', '81', '75', '75', '91', '64', '94', '64', '53', '67', '50', '54', '52', '41', '35', '32', '25', '27', '32', '50', '13', '26']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['97', '82', '76', '76', '92', '65', '95', '65', '54', '68', '51', '55', '53', '42', '36', '33', '26', '28', '33', '51', '14', '27']
[9.5s] Avg Throughput: propmt: 204, generation: 646
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['98', '83', '77', '77', '93', '66', '96', '66', '55', '69', '52', '56', '54', '43', '37', '34', '27', '29', '34', '52', '15', '28']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['99', '84', '78', '78', '94', '67', '97', '67', '56', '70', '53', '57', '55', '44', '38', '35', '28', '30', '35', '53', '16', '29']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['100', '85', '79', '79', '95', '68', '98', '68', '57', '71', '54', '58', '56', '45', '39', '36', '29', '31', '36', '54', '17', '30']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['101', '86', '80', '80', '96', '69', '99', '69', '58', '72', '55', '59', '57', '46', '40', '37', '30', '32', '37', '55', '18', '31']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['102', '87', '81', '81', '97', '70', '100', '70', '59', '73', '56', '60', '58', '47', '41', '38', '31', '33', '38', '56', '19', '32']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['103', '88', '82', '82', '98', '71', '101', '71', '60', '74', '57', '61', '59', '48', '42', '39', '32', '34', '39', '57', '20', '33']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['104', '89', '83', '83', '99', '72', '102', '72', '61', '75', '58', '62', '60', '49', '43', '40', '33', '35', '40', '58', '21', '34']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['105', '90', '84', '84', '100', '73', '103', '73', '62', '76', '59', '63', '61', '50', '44', '41', '34', '36', '41', '59', '22', '35']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['106', '91', '85', '85', '101', '74', '104', '74', '63', '77', '60', '64', '62', '51', '45', '42', '35', '37', '42', '60', '23', '36']
model: llama-7b, num requests: 22, total length: 22, prompt/kv_cache length: ['107', '92', '86', '86', '102', '75', '105', '75', '64', '78', '61', '65', '63', '52', '46', '43', '36', '38', '43', '61', '24', '37']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['108', '93', '87', '103', '76', '106', '76', '65', '79', '62', '66', '64', '53', '47', '44', '37', '39', '44', '62', '25', '38']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['109', '94', '88', '104', '77', '107', '77', '66', '80', '63', '67', '65', '54', '48', '45', '38', '40', '45', '63', '26', '39']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['110', '95', '89', '105', '78', '108', '78', '67', '81', '64', '68', '66', '55', '49', '46', '39', '41', '46', '64', '27', '40']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['111', '96', '90', '106', '79', '109', '79', '68', '82', '65', '69', '67', '56', '50', '47', '40', '42', '47', '65', '28', '41']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['112', '97', '91', '107', '80', '110', '80', '69', '83', '66', '70', '68', '57', '51', '48', '41', '43', '48', '66', '29', '42']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['113', '98', '92', '108', '81', '111', '81', '70', '84', '67', '71', '69', '58', '52', '49', '42', '44', '49', '67', '30', '43']
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['114', '99', '93', '109', '82', '112', '82', '71', '85', '68', '72', '70', '59', '53', '50', '43', '45', '50', '68', '31', '44']
[10.0s] Avg Throughput: propmt: 0, generation: 736
model: llama-7b, num requests: 21, total length: 21, prompt/kv_cache length: ['115', '100', '94', '110', '83', '113', '83', '72', '86', '69', '73', '71', '60', '54', '51', '44', '46', '51', '69', '32', '45']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['116', '101', '111', '84', '114', '84', '73', '87', '70', '74', '72', '61', '55', '52', '45', '47', '52', '70', '33', '46']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['117', '102', '112', '85', '115', '85', '74', '88', '71', '75', '73', '62', '56', '53', '46', '48', '53', '71', '34', '47']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['118', '103', '113', '86', '116', '86', '75', '89', '72', '76', '74', '63', '57', '54', '47', '49', '54', '72', '35', '48']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['119', '104', '114', '87', '117', '87', '76', '90', '73', '77', '75', '64', '58', '55', '48', '50', '55', '73', '36', '49']
model: llama-7b, num requests: 20, total length: 20, prompt/kv_cache length: ['120', '105', '115', '88', '118', '88', '77', '91', '74', '78', '76', '65', '59', '56', '49', '51', '56', '74', '37', '50']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['121', '106', '116', '89', '119', '89', '78', '92', '75', '79', '66', '60', '57', '50', '52', '57', '75', '38', '51']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['122', '107', '117', '90', '120', '90', '79', '93', '76', '80', '67', '61', '58', '51', '53', '58', '76', '39', '52']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['123', '108', '118', '91', '121', '91', '80', '94', '77', '81', '68', '62', '59', '52', '54', '59', '77', '40', '53']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['124', '109', '119', '92', '122', '92', '81', '95', '78', '82', '69', '63', '60', '53', '55', '60', '78', '41', '54']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['125', '110', '120', '93', '123', '93', '82', '96', '79', '83', '70', '64', '61', '54', '56', '61', '79', '42', '55']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['126', '111', '121', '94', '124', '94', '83', '97', '80', '84', '71', '65', '62', '55', '57', '62', '80', '43', '56']
model: llama-7b, num requests: 19, total length: 19, prompt/kv_cache length: ['127', '112', '122', '95', '125', '95', '84', '98', '81', '85', '72', '66', '63', '56', '58', '63', '81', '44', '57']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['128', '113', '123', '126', '96', '85', '99', '82', '86', '73', '67', '64', '57', '59', '64', '82', '45', '58']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['129', '114', '124', '127', '97', '86', '100', '83', '87', '74', '68', '65', '58', '60', '65', '83', '46', '59']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['130', '115', '125', '128', '98', '87', '101', '84', '88', '75', '69', '66', '59', '61', '66', '84', '47', '60']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['131', '116', '126', '129', '99', '88', '102', '85', '89', '76', '70', '67', '60', '62', '67', '85', '48', '61']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['132', '117', '127', '130', '100', '89', '103', '86', '90', '77', '71', '68', '61', '63', '68', '86', '49', '62']
[10.5s] Avg Throughput: propmt: 0, generation: 694
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['133', '118', '128', '131', '101', '90', '104', '87', '91', '78', '72', '69', '62', '64', '69', '87', '50', '63']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['134', '119', '129', '132', '102', '91', '105', '88', '92', '79', '73', '70', '63', '65', '70', '88', '51', '64']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['135', '120', '130', '133', '103', '92', '106', '89', '93', '80', '74', '71', '64', '66', '71', '89', '52', '65']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['136', '121', '131', '134', '104', '93', '107', '90', '94', '81', '75', '72', '65', '67', '72', '90', '53', '66']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['137', '122', '132', '135', '105', '94', '108', '91', '95', '82', '76', '73', '66', '68', '73', '91', '54', '67']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['138', '123', '133', '136', '106', '95', '109', '92', '96', '83', '77', '74', '67', '69', '74', '92', '55', '68']
model: llama-7b, num requests: 18, total length: 18, prompt/kv_cache length: ['139', '124', '134', '137', '107', '96', '110', '93', '97', '84', '78', '75', '68', '70', '75', '93', '56', '69']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['125', '135', '138', '108', '97', '94', '98', '85', '79', '76', '69', '71', '76', '94', '57', '70']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['126', '136', '139', '109', '98', '95', '99', '86', '80', '77', '70', '72', '77', '95', '58', '71']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['127', '137', '140', '110', '99', '96', '100', '87', '81', '78', '71', '73', '78', '96', '59', '72']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['128', '138', '141', '111', '100', '97', '101', '88', '82', '79', '72', '74', '79', '97', '60', '73']
model: llama-7b, num requests: 16, total length: 16, prompt/kv_cache length: ['129', '139', '142', '112', '101', '98', '102', '89', '83', '80', '73', '75', '80', '98', '61', '74']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['140', '143', '113', '102', '99', '103', '90', '84', '81', '74', '76', '81', '99', '62', '75']
model: llama-7b, num requests: 15, total length: 15, prompt/kv_cache length: ['141', '144', '114', '103', '100', '104', '91', '85', '82', '75', '77', '82', '100', '63', '76']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['145', '104', '101', '105', '92', '86', '83', '76', '78', '83', '101', '64', '77']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['146', '105', '102', '106', '93', '87', '84', '77', '79', '84', '102', '65', '78']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['147', '106', '103', '107', '94', '88', '85', '78', '80', '85', '103', '66', '79']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['148', '107', '104', '108', '95', '89', '86', '79', '81', '86', '104', '67', '80']
model: llama-7b, num requests: 13, total length: 13, prompt/kv_cache length: ['149', '108', '105', '109', '96', '90', '87', '80', '82', '87', '105', '68', '81']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['150', '109', '110', '97', '91', '88', '81', '83', '88', '106', '69', '82']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['151', '110', '111', '98', '92', '89', '82', '84', '89', '107', '70', '83']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['152', '111', '112', '99', '93', '90', '83', '85', '90', '108', '71', '84']
[11.0s] Avg Throughput: propmt: 0, generation: 686
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['153', '112', '113', '100', '94', '91', '84', '86', '91', '109', '72', '85']
model: llama-7b, num requests: 12, total length: 12, prompt/kv_cache length: ['154', '113', '114', '101', '95', '92', '85', '87', '92', '110', '73', '86']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['155', '114', '115', '102', '96', '93', '86', '88', '93', '111', '87']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['156', '115', '116', '103', '97', '94', '87', '89', '94', '112', '88']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['157', '116', '117', '104', '98', '95', '88', '90', '95', '113', '89']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['158', '117', '118', '105', '99', '96', '89', '91', '96', '114', '90']
model: llama-7b, num requests: 11, total length: 11, prompt/kv_cache length: ['159', '118', '119', '106', '100', '97', '90', '92', '97', '115', '91']
model: llama-7b, num requests: 10, total length: 10, prompt/kv_cache length: ['160', '119', '107', '101', '98', '91', '93', '98', '116', '92']
model: llama-7b, num requests: 10, total length: 10, prompt/kv_cache length: ['161', '120', '108', '102', '99', '92', '94', '99', '117', '93']
model: llama-7b, num requests: 10, total length: 10, prompt/kv_cache length: ['162', '121', '109', '103', '100', '93', '95', '100', '118', '94']
model: llama-7b, num requests: 10, total length: 10, prompt/kv_cache length: ['163', '122', '110', '104', '101', '94', '96', '101', '119', '95']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['123', '111', '105', '102', '95', '97', '102', '120', '96']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['124', '112', '106', '103', '96', '98', '103', '121', '97']
model: llama-7b, num requests: 9, total length: 9, prompt/kv_cache length: ['125', '113', '107', '104', '97', '99', '104', '122', '98']
model: llama-7b, num requests: 8, total length: 8, prompt/kv_cache length: ['126', '114', '105', '98', '100', '105', '123', '99']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['127', '115', '106', '99', '101', '124', '100']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['128', '116', '107', '100', '102', '125', '101']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['129', '117', '108', '101', '103', '126', '102']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['130', '118', '109', '102', '104', '127', '103']
model: llama-7b, num requests: 7, total length: 7, prompt/kv_cache length: ['131', '119', '110', '103', '105', '128', '104']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['120', '111', '104', '106', '129', '105']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['121', '112', '105', '107', '130', '106']
model: llama-7b, num requests: 6, total length: 6, prompt/kv_cache length: ['122', '113', '106', '108', '131', '107']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['114', '107', '109', '132', '108']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['115', '108', '110', '133', '109']
model: llama-7b, num requests: 5, total length: 5, prompt/kv_cache length: ['116', '109', '111', '134', '110']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['110', '112', '135', '111']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['111', '113', '136', '112']
model: llama-7b, num requests: 4, total length: 4, prompt/kv_cache length: ['112', '114', '137', '113']
model: llama-7b, num requests: 3, total length: 3, prompt/kv_cache length: ['115', '138', '114']
model: llama-7b, num requests: 3, total length: 3, prompt/kv_cache length: ['116', '139', '115']
model: llama-7b, num requests: 3, total length: 3, prompt/kv_cache length: ['117', '140', '116']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['141', '117']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['142', '118']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['143', '119']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['144', '120']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['145', '121']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['146', '122']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['147', '123']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['148', '124']
model: llama-7b, num requests: 2, total length: 2, prompt/kv_cache length: ['149', '125']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['150']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['151']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['152']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['153']
model: llama-7b, num requests: 1, total length: 1, prompt/kv_cache length: ['154']
[11.5s] Avg Throughput: propmt: 0, generation: 556
---------------------------
Exiting The Simulator
Memory Is All Freed
Checking Non-Exited Systems ...
---------------------------
All Request Has Been Exited
---------------------------
{'id': 0, 'model': 'llama-7b', 'input': 10, 'output': 80, 'arrival': 0, 'end_time': 275578867, 'latency': 275578867}
{'id': 3, 'model': 'llama-7b', 'input': 4, 'output': 16, 'arrival': 570907776, 'end_time': 701681888, 'latency': 130774112}
{'id': 7, 'model': 'llama-7b', 'input': 2, 'output': 11, 'arrival': 811936856, 'end_time': 911761028, 'latency': 99824172}
{'id': 2, 'model': 'llama-7b', 'input': 16, 'output': 85, 'arrival': 479613521, 'end_time': 1173515994, 'latency': 693902473}
{'id': 1, 'model': 'llama-7b', 'input': 22, 'output': 126, 'arrival': 347938952, 'end_time': 1208762642, 'latency': 860823690}
{'id': 5, 'model': 'llama-7b', 'input': 15, 'output': 78, 'arrival': 604829893, 'end_time': 1330248618, 'latency': 725418725}
{'id': 9, 'model': 'llama-7b', 'input': 5, 'output': 31, 'arrival': 1026970077, 'end_time': 1366878874, 'latency': 339908797}
{'id': 6, 'model': 'llama-7b', 'input': 15, 'output': 99, 'arrival': 610813769, 'end_time': 1529840481, 'latency': 919026712}
{'id': 8, 'model': 'llama-7b', 'input': 18, 'output': 76, 'arrival': 903845071, 'end_time': 1553542320, 'latency': 649697249}
{'id': 4, 'model': 'llama-7b', 'input': 13, 'output': 109, 'arrival': 587870263, 'end_time': 1614855988, 'latency': 1026985725}
{'id': 10, 'model': 'llama-7b', 'input': 83, 'output': 147, 'arrival': 1029050008, 'end_time': 1755231597, 'latency': 726181589}
{'id': 21, 'model': 'llama-7b', 'input': 10, 'output': 20, 'arrival': 1933551216, 'end_time': 2168672457, 'latency': 235121241}
{'id': 18, 'model': 'llama-7b', 'input': 30, 'output': 51, 'arrival': 1789468376, 'end_time': 2207253609, 'latency': 417785233}
{'id': 25, 'model': 'llama-7b', 'input': 10, 'output': 20, 'arrival': 2228417559, 'end_time': 2502889977, 'latency': 274472418}
{'id': 20, 'model': 'llama-7b', 'input': 8, 'output': 37, 'arrival': 1918527763, 'end_time': 2639665640, 'latency': 721137877}
{'id': 16, 'model': 'llama-7b', 'input': 23, 'output': 83, 'arrival': 1658521887, 'end_time': 3024364620, 'latency': 1365842733}
{'id': 11, 'model': 'llama-7b', 'input': 8, 'output': 99, 'arrival': 1379405756, 'end_time': 3050395501, 'latency': 1670989745}
{'id': 19, 'model': 'llama-7b', 'input': 25, 'output': 79, 'arrival': 1823890676, 'end_time': 3147510485, 'latency': 1323619809}
{'id': 33, 'model': 'llama-7b', 'input': 43, 'output': 56, 'arrival': 2833725517, 'end_time': 3202425340, 'latency': 368699823}
{'id': 22, 'model': 'llama-7b', 'input': 4, 'output': 54, 'arrival': 1968102767, 'end_time': 3272827704, 'latency': 1304724937}
{'id': 28, 'model': 'llama-7b', 'input': 47, 'output': 78, 'arrival': 2412644533, 'end_time': 3295031765, 'latency': 882387232}
{'id': 31, 'model': 'llama-7b', 'input': 8, 'output': 35, 'arrival': 2529630345, 'end_time': 3295031765, 'latency': 765401420}
{'id': 12, 'model': 'llama-7b', 'input': 14, 'output': 100, 'arrival': 1558048710, 'end_time': 3397119558, 'latency': 1839070848}
{'id': 40, 'model': 'llama-7b', 'input': 103, 'output': 105, 'arrival': 3568927610, 'end_time': 3728583083, 'latency': 159655473}
{'id': 13, 'model': 'llama-7b', 'input': 14, 'output': 113, 'arrival': 1581917473, 'end_time': 3888927925, 'latency': 2307010452}
{'id': 14, 'model': 'llama-7b', 'input': 3, 'output': 107, 'arrival': 1601985371, 'end_time': 4075528143, 'latency': 2473542772}
{'id': 36, 'model': 'llama-7b', 'input': 1, 'output': 28, 'arrival': 3372350495, 'end_time': 4327465629, 'latency': 955115134}
{'id': 38, 'model': 'llama-7b', 'input': 2, 'output': 23, 'arrival': 3497903303, 'end_time': 4327465629, 'latency': 829562326}
{'id': 39, 'model': 'llama-7b', 'input': 1, 'output': 21, 'arrival': 3555912387, 'end_time': 4358794487, 'latency': 802882100}
{'id': 24, 'model': 'llama-7b', 'input': 7, 'output': 95, 'arrival': 2074623958, 'end_time': 4636147818, 'latency': 2561523860}
{'id': 15, 'model': 'llama-7b', 'input': 8, 'output': 127, 'arrival': 1622246514, 'end_time': 4724912982, 'latency': 3102666468}
{'id': 27, 'model': 'llama-7b', 'input': 8, 'output': 91, 'arrival': 2322894061, 'end_time': 4844619860, 'latency': 2521725799}
{'id': 35, 'model': 'llama-7b', 'input': 8, 'output': 55, 'arrival': 3336021709, 'end_time': 4901507248, 'latency': 1565485539}
{'id': 43, 'model': 'llama-7b', 'input': 17, 'output': 47, 'arrival': 3880823994, 'end_time': 4929140935, 'latency': 1048316941}
{'id': 45, 'model': 'llama-7b', 'input': 20, 'output': 46, 'arrival': 4019395352, 'end_time': 4955505640, 'latency': 936110288}
{'id': 51, 'model': 'llama-7b', 'input': 4, 'output': 13, 'arrival': 4728227590, 'end_time': 5003839804, 'latency': 275612214}
{'id': 17, 'model': 'llama-7b', 'input': 18, 'output': 140, 'arrival': 1732914670, 'end_time': 5059152364, 'latency': 3326237694}
{'id': 37, 'model': 'llama-7b', 'input': 9, 'output': 60, 'arrival': 3382628226, 'end_time': 5059152364, 'latency': 1676524138}
{'id': 34, 'model': 'llama-7b', 'input': 17, 'output': 84, 'arrival': 3170788552, 'end_time': 5194277594, 'latency': 2023489042}
{'id': 26, 'model': 'llama-7b', 'input': 16, 'output': 117, 'arrival': 2250691145, 'end_time': 5214614368, 'latency': 2963923223}
{'id': 23, 'model': 'llama-7b', 'input': 68, 'output': 181, 'arrival': 2013730489, 'end_time': 5234521392, 'latency': 3220790903}
{'id': 49, 'model': 'llama-7b', 'input': 73, 'output': 112, 'arrival': 4229722339, 'end_time': 5360418659, 'latency': 1130696320}
{'id': 52, 'model': 'llama-7b', 'input': 16, 'output': 35, 'arrival': 5008737032, 'end_time': 5459304307, 'latency': 450567275}
{'id': 30, 'model': 'llama-7b', 'input': 26, 'output': 129, 'arrival': 2510934220, 'end_time': 5477846604, 'latency': 2966912384}
{'id': 32, 'model': 'llama-7b', 'input': 43, 'output': 155, 'arrival': 2536356738, 'end_time': 5832340622, 'latency': 3295983884}
{'id': 56, 'model': 'llama-7b', 'input': 91, 'output': 95, 'arrival': 5589266757, 'end_time': 5857276120, 'latency': 268009363}
{'id': 58, 'model': 'llama-7b', 'input': 24, 'output': 27, 'arrival': 5615708423, 'end_time': 5857276120, 'latency': 241567697}
{'id': 29, 'model': 'llama-7b', 'input': 13, 'output': 131, 'arrival': 2417400918, 'end_time': 5878713077, 'latency': 3461312159}
{'id': 44, 'model': 'llama-7b', 'input': 6, 'output': 71, 'arrival': 3910769772, 'end_time': 5878713077, 'latency': 1967943305}
{'id': 41, 'model': 'llama-7b', 'input': 63, 'output': 150, 'arrival': 3637282333, 'end_time': 6235160606, 'latency': 2597878273}
{'id': 53, 'model': 'llama-7b', 'input': 49, 'output': 87, 'arrival': 5233952232, 'end_time': 6259955560, 'latency': 1026003328}
{'id': 48, 'model': 'llama-7b', 'input': 9, 'output': 90, 'arrival': 4209283479, 'end_time': 6510337161, 'latency': 2301053682}
{'id': 55, 'model': 'llama-7b', 'input': 39, 'output': 70, 'arrival': 5580001211, 'end_time': 6535173011, 'latency': 955171800}
{'id': 50, 'model': 'llama-7b', 'input': 12, 'output': 85, 'arrival': 4579003052, 'end_time': 6558715623, 'latency': 1979712571}
{'id': 63, 'model': 'llama-7b', 'input': 6, 'output': 29, 'arrival': 5956509192, 'end_time': 6580948996, 'latency': 624439804}
{'id': 57, 'model': 'llama-7b', 'input': 24, 'output': 57, 'arrival': 5611080226, 'end_time': 6601926858, 'latency': 990846632}
{'id': 42, 'model': 'llama-7b', 'input': 5, 'output': 124, 'arrival': 3640781705, 'end_time': 7036965831, 'latency': 3396184126}
{'id': 47, 'model': 'llama-7b', 'input': 109, 'output': 213, 'arrival': 4130161099, 'end_time': 7036965831, 'latency': 2906804732}
{'id': 69, 'model': 'llama-7b', 'input': 7, 'output': 21, 'arrival': 6686130579, 'end_time': 7064566236, 'latency': 378435657}
{'id': 73, 'model': 'llama-7b', 'input': 11, 'output': 12, 'arrival': 7025776321, 'end_time': 7064566236, 'latency': 38789915}
{'id': 54, 'model': 'llama-7b', 'input': 9, 'output': 93, 'arrival': 5325057673, 'end_time': 7493446635, 'latency': 2168388962}
{'id': 46, 'model': 'llama-7b', 'input': 36, 'output': 162, 'arrival': 4056750010, 'end_time': 7536043062, 'latency': 3479293052}
{'id': 61, 'model': 'llama-7b', 'input': 19, 'output': 92, 'arrival': 5735930705, 'end_time': 7639468794, 'latency': 1903538089}
{'id': 66, 'model': 'llama-7b', 'input': 19, 'output': 80, 'arrival': 6082919999, 'end_time': 7639468794, 'latency': 1556548795}
{'id': 62, 'model': 'llama-7b', 'input': 14, 'output': 87, 'arrival': 5912386492, 'end_time': 7780858076, 'latency': 1868471584}
{'id': 68, 'model': 'llama-7b', 'input': 4, 'output': 66, 'arrival': 6252715945, 'end_time': 7832639692, 'latency': 1579923747}
{'id': 70, 'model': 'llama-7b', 'input': 49, 'output': 105, 'arrival': 6834078957, 'end_time': 8445201704, 'latency': 1611122747}
{'id': 60, 'model': 'llama-7b', 'input': 18, 'output': 122, 'arrival': 5704274661, 'end_time': 8626591210, 'latency': 2922316549}
{'id': 67, 'model': 'llama-7b', 'input': 36, 'output': 127, 'arrival': 6244968358, 'end_time': 8800500439, 'latency': 2555532081}
{'id': 72, 'model': 'llama-7b', 'input': 31, 'output': 98, 'arrival': 6856786643, 'end_time': 8872298491, 'latency': 2015511848}
{'id': 86, 'model': 'llama-7b', 'input': 1, 'output': 25, 'arrival': 8041570457, 'end_time': 8906135011, 'latency': 864564554}
{'id': 89, 'model': 'llama-7b', 'input': 9, 'output': 23, 'arrival': 8425191197, 'end_time': 8938728628, 'latency': 513537431}
{'id': 59, 'model': 'llama-7b', 'input': 120, 'output': 238, 'arrival': 5655061632, 'end_time': 9081797763, 'latency': 3426736131}
{'id': 64, 'model': 'llama-7b', 'input': 27, 'output': 135, 'arrival': 5989489476, 'end_time': 9081797763, 'latency': 3092308287}
{'id': 93, 'model': 'llama-7b', 'input': 7, 'output': 15, 'arrival': 8788269096, 'end_time': 9081797763, 'latency': 293528667}
{'id': 65, 'model': 'llama-7b', 'input': 12, 'output': 120, 'arrival': 6067730184, 'end_time': 9152049809, 'latency': 3084319625}
{'id': 88, 'model': 'llama-7b', 'input': 66, 'output': 94, 'arrival': 8361284584, 'end_time': 9412885534, 'latency': 1051600950}
{'id': 74, 'model': 'llama-7b', 'input': 22, 'output': 94, 'arrival': 7148485911, 'end_time': 9445294959, 'latency': 2296809048}
{'id': 76, 'model': 'llama-7b', 'input': 14, 'output': 87, 'arrival': 7426573655, 'end_time': 9814344359, 'latency': 2387770704}
{'id': 77, 'model': 'llama-7b', 'input': 15, 'output': 95, 'arrival': 7434266581, 'end_time': 10051479623, 'latency': 2617213042}
{'id': 87, 'model': 'llama-7b', 'input': 16, 'output': 77, 'arrival': 8143059393, 'end_time': 10194144781, 'latency': 2051085388}
{'id': 79, 'model': 'llama-7b', 'input': 5, 'output': 96, 'arrival': 7490970859, 'end_time': 10386163182, 'latency': 2895192323}
{'id': 71, 'model': 'llama-7b', 'input': 13, 'output': 140, 'arrival': 6856232901, 'end_time': 10699068730, 'latency': 3842835829}
{'id': 83, 'model': 'llama-7b', 'input': 21, 'output': 111, 'arrival': 7834202622, 'end_time': 10699068730, 'latency': 2864866108}
{'id': 75, 'model': 'llama-7b', 'input': 14, 'output': 130, 'arrival': 7279052202, 'end_time': 10814230942, 'latency': 3535178740}
{'id': 78, 'model': 'llama-7b', 'input': 32, 'output': 142, 'arrival': 7478655849, 'end_time': 10859509016, 'latency': 3380853167}
{'id': 81, 'model': 'llama-7b', 'input': 16, 'output': 115, 'arrival': 7787453935, 'end_time': 10859509016, 'latency': 3072055081}
{'id': 84, 'model': 'llama-7b', 'input': 5, 'output': 106, 'arrival': 7871451457, 'end_time': 10958317288, 'latency': 3086865831}
{'id': 98, 'model': 'llama-7b', 'input': 5, 'output': 74, 'arrival': 9136063646, 'end_time': 11050731557, 'latency': 1914667911}
{'id': 85, 'model': 'llama-7b', 'input': 10, 'output': 120, 'arrival': 7910782878, 'end_time': 11134960980, 'latency': 3224178102}
{'id': 80, 'model': 'llama-7b', 'input': 43, 'output': 164, 'arrival': 7689823816, 'end_time': 11198408136, 'latency': 3508584320}
{'id': 91, 'model': 'llama-7b', 'input': 11, 'output': 108, 'arrival': 8562841071, 'end_time': 11240707786, 'latency': 2677866715}
{'id': 96, 'model': 'llama-7b', 'input': 22, 'output': 106, 'arrival': 9077707450, 'end_time': 11254102698, 'latency': 2176395248}
{'id': 82, 'model': 'llama-7b', 'input': 6, 'output': 132, 'arrival': 7827635815, 'end_time': 11314472690, 'latency': 3486836875}
{'id': 90, 'model': 'llama-7b', 'input': 13, 'output': 123, 'arrival': 8437928437, 'end_time': 11346661864, 'latency': 2908733427}
{'id': 92, 'model': 'llama-7b', 'input': 12, 'output': 117, 'arrival': 8705880346, 'end_time': 11374815484, 'latency': 2668935138}
{'id': 94, 'model': 'llama-7b', 'input': 11, 'output': 113, 'arrival': 8935658093, 'end_time': 11398941963, 'latency': 2463283870}
{'id': 95, 'model': 'llama-7b', 'input': 15, 'output': 118, 'arrival': 9003739566, 'end_time': 11419044440, 'latency': 2415304874}
{'id': 99, 'model': 'llama-7b', 'input': 18, 'output': 126, 'arrival': 9147480390, 'end_time': 11467262576, 'latency': 2319782186}
{'id': 97, 'model': 'llama-7b', 'input': 42, 'output': 155, 'arrival': 9133488869, 'end_time': 11487285338, 'latency': 2353796469}
---------------------------
Throughput Results
---------------------------
Total prompts: 2241 tokens/s
Total Generation: 6706 tokens/s
Throughput per 0.5 sec: [(96, 206), (134, 492), (192, 516), (314, 508), (338, 538), (240, 646), (70, 650), (394, 536), (494, 500), (32, 688), (148, 636), (710, 414), (196, 620), (200, 610), (152, 642), (276, 584), (210, 638), (82, 670), (204, 646), (0, 736), (0, 694), (0, 686), (0, 556)]
Total clocks: 11487285338 ticks
Total latency: 11.487285338 s
Average throughput: prompt: 195.0852559208885 generation: 583.7758706851755
---------------------------
Simulation Time (ms)
---------------------------
Total execution engine time: 241352.898
Total graph time: 78636.719
Total astra time: 45160.929
Total scheduler time: 8234.527
Total simulation time: 373385.073
